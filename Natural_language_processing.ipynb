{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Natural_language_processing.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1gTeumJ0yTFRUOzEAQdg0CkdV_Caux2Sl","authorship_tag":"ABX9TyNpmTIKUFH4/RYQWNALsO8O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6R8Y8TtjYds1"},"source":["1.このSprintについて\n","\n","Sprintの目的\n","自然言語処理の一連の流れを学ぶ\n","自然言語のベクトル化の方法を学ぶ\n","\n","どのように学ぶか\n","自然言語処理定番のデータセットを用いて、一連の流れを見ていきます。"]},{"cell_type":"markdown","metadata":{"id":"BLFmuHY6ZAUc"},"source":["2.自然言語のベクトル化\n","\n","自然言語処理（NLP, Natural Language Processing） とは人間が普段使っている 自然言語 をコンピュータに処理させる技術のことです。ここではその中でも、機械学習の入力として自然言語を用いることを考えていきます。\n","\n","\n","多くの機械学習手法は 数値データ（量的変数） の入力を前提にしていますので、自然言語の テキストデータ を数値データに変換する必要があります。これを 自然言語のベクトル化 と呼びます。ベクトル化の際にテキストデータの特徴をうまく捉えられるよう、さまざまな手法が考えられてきていますので、このSprintではそれらを学びます。\n","\n","\n","非構造化データ\n","データの分類として、表に数値がまとめられたようなコンピュータが扱いやすい形を 構造化データ 、人間が扱いやすい画像・動画・テキスト・音声などを 非構造化データ と呼ぶことがあります。自然言語のベクトル化は、非構造化データを構造化データに変換する工程と言えます。同じ非構造化データでも、画像に対してはディープラーニングを用いる場合この変換作業はあまり必要がありませんでしたが、テキストにおいてはこれをどう行うかが重要です。\n","\n","\n","自然言語処理により何ができるか\n","機械学習の入力や出力に自然言語のテキストを用いることでさまざまなことができます。入力も出力もテキストである例としては 機械翻訳 があげられ、実用化されています。入力は画像で出力がテキストである 画像キャプション生成 やその逆の文章からの画像生成も研究が進んでいます。\n","\n","\n","しかし、出力をテキストや画像のような非構造化データとすることは難易度が高いです。比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする テキスト分類 です。\n","\n","\n","アヤメやタイタニック、手書き数字のような定番の存在として、IMDB映画レビューデータセット の感情分析があります。レビューの文書が映画に対して肯定的か否定的かを2値分類します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。\n","\n","\n","3.IMDB映画レビューデータセットの準備\n","\n","IMDB映画レビューデータセットを準備します。\n","\n","\n","ダウンロード\n","次のwgetコマンドによってダウンロードします。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","# IMDBをカレントフォルダにダウンロード\n","!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","# 解凍\n","!tar zxf aclImdb_v1.tar.gz\n","# aclImdb/train/unsupはラベル無しのため削除\n","!rm -rf aclImdb/train/unsup\n","# IMDBデータセットの説明を表示\n","!cat aclImdb/README\n","\n","以下のサイトで公開されているデータセットです。\n","\n","\n","Sentiment Analysis\n","\n","\n","読み込み\n","scikit-learnのload_filesを用いて読み込みます。\n","\n","\n","sklearn.datasets.load_files — scikit-learn 0.21.3 documentation\n","\n","\n","《読み込むコード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","from sklearn.datasets import load_files\n","train_review = load_files('./aclImdb/train/', encoding='utf-8')\n","x_train, y_train = train_review.data, train_review.target\n","test_review = load_files('./aclImdb/test/', encoding='utf-8')\n","x_test, y_test = test_review.data, test_review.target\n","# ラベルの0,1と意味の対応の表示\n","print(train_review.target_names)\n","\n","このデータセットについて\n","中身を見てみると、英語の文章が入っていることが分かります。\n","\n","\n","1\n","print(\"x : {}\".format(x_train[0]))\n","\n","IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n","\n","\n","Ratings and Reviews for New Movies and TV Shows - IMDb\n","\n","\n","このサイトではユーザが映画に対して1から10点の評価とコメントを投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n","\n","\n","4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付けしており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。\n","\n","\n","4.古典的な手法\n","\n","古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。\n","\n","\n","5.BoW\n","\n","単純ながら効果的な方法として BoW (Bag of Words) があります。これは、サンプルごとに単語などの 登場回数 を数えたものをベクトルとする方法です。単語をカテゴリとして捉え one-hot表現 していることになります。\n","\n","\n","例\n","例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。\n","\n","\n","1\n","2\n","3\n","4\n","mini_dataset = \\\n","  [\"This movie is very good.\",\n","  \"This film is a good\",\n","  \"Very bad. Very, very bad.\"]\n","\n","この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n","\n","\n","sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n","bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n","# DataFrameにまとめる\n","df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n","display(df)\n","\n","実行すると次のような表が得られます。\n","\n","\n","Image from Gyazo\n","\n","\n","例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ 語彙 と呼びます。\n","\n","\n","テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを コーパス と呼びます。語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。\n","\n","\n","前処理\n","CountVectorizerクラスでは大文字は小文字に揃えるという 前処理 が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（テキストクリーニング）や表記揺れの統一といったことを別途行うことが一般的です。\n","\n","\n","語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える ステミング と呼ばれる処理を行うこともあります。\n","\n","\n","トークン\n","BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。\n","\n","\n","何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n","\n","\n","デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。\n","\n","\n","《正規表現》\n","\n","\n","正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n","\n","\n","re — 正規表現操作\n","\n","\n","正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n","\n","\n","Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript\n","\n","\n","形態素解析\n","英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n","\n","\n","日本語では名詞や助詞、動詞のように異なる 品詞 で分けられる単位で 分かち書き することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n","\n","\n","これには MeCab や Janome のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb茶まめというサービスも国立国語研究所が提供しています。\n","\n","\n","自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として mecab-ipadic-NEologd がオープンソースで存在しています。\n","\n","\n","mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd\n","\n","\n","n-gram\n","上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順はまったく考慮されていません。\n","\n","\n","考慮するために、隣あう単語同士をまとめて扱う n-gram という考え方を適用することがあります。2つの単語をまとめる場合は 2-gram (bigram) と呼び、次のようになります。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","# ngram_rangeで利用するn-gramの範囲を指定する\n","vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n","bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n","df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n","display(df)\n","\n","Image from Gyazo\n","\n","\n","2-gramにより「very good」と「very bad」が区別して数えられています。\n","\n","\n","単語をまとめない場合は 1-gram (unigram) と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。\n"]},{"cell_type":"markdown","metadata":{"id":"sW-PkVtvYfay"},"source":["\n","\n","【問題1】BoWのスクラッチ実装\n","以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n","\n","\n","This movie is SOOOO funny!!!\n","\n","What a movie! I never\n","\n","best movie ever!!!!! this movie\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqQcFsS4Z1St","executionInfo":{"status":"ok","timestamp":1631177805290,"user_tz":-540,"elapsed":17854,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"abea8304-7d19-4942-e095-b9da0e42f5c7"},"source":["# IMDBをカレントフォルダにダウンロード\n","!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","# 解凍\n","!tar zxf aclImdb_v1.tar.gz\n","# aclImdb/train/unsupはラベル無しのため削除\n","!rm -rf aclImdb/train/unsup\n","# IMDBデータセットの説明を表示\n","!cat aclImdb/README"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-09-09 08:56:27--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  8.41MB/s    in 8.2s    \n","\n","2021-09-09 08:56:35 (9.74 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n","Large Movie Review Dataset v1.0\n","\n","Overview\n","\n","This dataset contains movie reviews along with their associated binary\n","sentiment polarity labels. It is intended to serve as a benchmark for\n","sentiment classification. This document outlines how the dataset was\n","gathered, and how to use the files provided. \n","\n","Dataset \n","\n","The core dataset contains 50,000 reviews split evenly into 25k train\n","and 25k test sets. The overall distribution of labels is balanced (25k\n","pos and 25k neg). We also include an additional 50,000 unlabeled\n","documents for unsupervised learning. \n","\n","In the entire collection, no more than 30 reviews are allowed for any\n","given movie because reviews for the same movie tend to have correlated\n","ratings. Further, the train and test sets contain a disjoint set of\n","movies, so no significant performance is obtained by memorizing\n","movie-unique terms and their associated with observed labels.  In the\n","labeled train/test sets, a negative review has a score <= 4 out of 10,\n","and a positive review has a score >= 7 out of 10. Thus reviews with\n","more neutral ratings are not included in the train/test sets. In the\n","unsupervised set, reviews of any rating are included and there are an\n","even number of reviews > 5 and <= 5.\n","\n","Files\n","\n","There are two top-level directories [train/, test/] corresponding to\n","the training and test sets. Each contains [pos/, neg/] directories for\n","the reviews with binary labels positive and negative. Within these\n","directories, reviews are stored in text files named following the\n","convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n","the star rating for that review on a 1-10 scale. For example, the file\n","[test/pos/200_8.txt] is the text for a positive-labeled test set\n","example with unique id 200 and star rating 8/10 from IMDb. The\n","[train/unsup/] directory has 0 for all ratings because the ratings are\n","omitted for this portion of the dataset.\n","\n","We also include the IMDb URLs for each review in a separate\n","[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n","have its URL on line 200 of this file. Due the ever-changing IMDb, we\n","are unable to link directly to the review, but only to the movie's\n","review page.\n","\n","In addition to the review text files, we include already-tokenized bag\n","of words (BoW) features that were used in our experiments. These \n","are stored in .feat files in the train/test directories. Each .feat\n","file is in LIBSVM format, an ascii sparse-vector format for labeled\n","data.  The feature indices in these files start from 0, and the text\n","tokens corresponding to a feature index is found in [imdb.vocab]. So a\n","line with 0:7 in a .feat file means the first word in [imdb.vocab]\n","(the) appears 7 times in that review.\n","\n","LIBSVM page for details on .feat file format:\n","http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n","\n","We also include [imdbEr.txt] which contains the expected rating for\n","each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n","rating is a good way to get a sense for the average polarity of a word\n","in the dataset.\n","\n","Citing the dataset\n","\n","When using this dataset please cite our ACL 2011 paper which\n","introduces it. This paper also contains classification results which\n","you may want to compare against.\n","\n","\n","@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n","  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n","  title     = {Learning Word Vectors for Sentiment Analysis},\n","  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n","  month     = {June},\n","  year      = {2011},\n","  address   = {Portland, Oregon, USA},\n","  publisher = {Association for Computational Linguistics},\n","  pages     = {142--150},\n","  url       = {http://www.aclweb.org/anthology/P11-1015}\n","}\n","\n","References\n","\n","Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n","David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n","636-659.\n","\n","Contact\n","\n","For questions/comments/corrections please contact Andrew Maas\n","amaas@cs.stanford.edu\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGrSYV58eRHC","executionInfo":{"status":"ok","timestamp":1631177806837,"user_tz":-540,"elapsed":1551,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"08a505b5-5c39-4bf0-b943-e4258f749833"},"source":["from sklearn.datasets import load_files\n","train_review = load_files('./aclImdb/train/', encoding='utf-8')\n","x_train, y_train = train_review.data, train_review.target\n","test_review = load_files('./aclImdb/test/', encoding='utf-8')\n","x_test, y_test = test_review.data, test_review.target\n","# ラベルの0,1と意味の対応の表示\n","print(train_review.target_names)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['neg', 'pos']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVngTyMBeRLr","executionInfo":{"status":"ok","timestamp":1631177806838,"user_tz":-540,"elapsed":22,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"6dcdc7cc-b650-4fef-bbd6-d7a9759f70c6"},"source":["print(\"x : {}\".format(x_train[0]))\n"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"]}]},{"cell_type":"code","metadata":{"id":"NWwvG5zngLpm","executionInfo":{"status":"ok","timestamp":1631177806838,"user_tz":-540,"elapsed":19,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":["mini_dataset = \\\n","  [\"This movie is very good.\",\n","  \"This film is a good\",\n","  \"Very bad. Very, very bad.\"]"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"RqVNGQKIeRo1","executionInfo":{"status":"ok","timestamp":1631177806839,"user_tz":-540,"elapsed":19,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"463f2e44-d475-4b7e-e2dd-525580b1a433"},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n","bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n","# DataFrameにまとめる\n","df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n","display(df)"],"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>a</th>\n","      <th>bad</th>\n","      <th>film</th>\n","      <th>good</th>\n","      <th>is</th>\n","      <th>movie</th>\n","      <th>this</th>\n","      <th>very</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   a  bad  film  good  is  movie  this  very\n","0  0    0     0     1   1      1     1     1\n","1  1    0     1     1   1      0     1     0\n","2  0    2     0     0   0      0     0     3"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3Rxk6l7eR1f","executionInfo":{"status":"ok","timestamp":1631178486615,"user_tz":-540,"elapsed":222,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"e905e3e0-c5c2-4e1c-9847-4d9104acc336"},"source":["\n","\n","sentence = \\\n","\"This movie is SOOOO funny!!!\" ' '\\\n","\"What a movie! I never\" ' '\\\n","\"best movie ever!!!!! this movie\"\n","print(sentence)\n","\n","def word_n_gram(sentence, N):\n","    \"\"\"\n","    単語のn-gramを返す。\n","    \"\"\"\n","    words = sentence.split()\n","    #print(words)\n","    result = []\n","    for it, c in enumerate(words):\n","        if it + N > len(words):\n","          # DataFrameにまとめる\n","          # df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n","          # display(df)\n","          return result\n","        result.append(words[it: it+N])\n","\n","      \n","    "],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["This movie is SOOOO funny!!! What a movie! I never best movie ever!!!!! this movie\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Z6Vl3ZIeR6y","executionInfo":{"status":"ok","timestamp":1631178658206,"user_tz":-540,"elapsed":221,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"39c84250-8914-44e7-d552-cc1920485cea"},"source":["#1-gram\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import re\n","\n","sentence = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sentence) #特殊文字削除\n","word = word_n_gram(sentence, N=1)\n","\n","print(word)\n","\n","if(word==None):\n","  pass\n","else:\n","  vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n","  bow = (vectorizer.fit_transform(words)).toarray()  \n","  df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n","  display(df)\n","\n"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":830},"id":"OMEiYhTaRxaK","executionInfo":{"status":"ok","timestamp":1631178621735,"user_tz":-540,"elapsed":358,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"f5ba5806-307f-4127-9b8c-64109cb38da1"},"source":["#2-gram\n","#print(sentence)\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import re\n","\n","sentence = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sentence) #特殊文字削除\n","# print(word_n_gram(sentence, N=2))\n","word = word_n_gram(sentence, N=2)\n","\n","print(word)\n","\n","if(word==None):\n","  pass\n","else:\n","  vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n","  bow = (vectorizer.fit_transform(words)).toarray()  \n","  df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n","  display(df)"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["[['This', 'movie'], ['movie', 'is'], ['is', 'SOOOO'], ['SOOOO', 'funny'], ['funny', 'What'], ['What', 'a'], ['a', 'movie'], ['movie', 'I'], ['I', 'never'], ['never', 'best'], ['best', 'movie'], ['movie', 'ever'], ['ever', 'this'], ['this', 'movie']]\n"]},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>a</th>\n","      <th>best</th>\n","      <th>ever</th>\n","      <th>funny</th>\n","      <th>i</th>\n","      <th>is</th>\n","      <th>movie</th>\n","      <th>never</th>\n","      <th>soooo</th>\n","      <th>this</th>\n","      <th>what</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    a  best  ever  funny  i  is  movie  never  soooo  this  what\n","0   0     0     0      0  0   0      0      0      0     1     0\n","1   0     0     0      0  0   0      1      0      0     0     0\n","2   0     0     0      0  0   1      0      0      0     0     0\n","3   0     0     0      0  0   0      0      0      1     0     0\n","4   0     0     0      1  0   0      0      0      0     0     0\n","5   0     0     0      0  0   0      0      0      0     0     0\n","6   0     0     0      0  0   0      0      0      0     0     0\n","7   0     0     0      0  0   0      0      0      0     0     0\n","8   0     0     0      0  0   0      0      0      0     0     1\n","9   1     0     0      0  0   0      0      0      0     0     0\n","10  0     0     0      0  0   0      1      0      0     0     0\n","11  0     0     0      0  0   0      0      0      0     0     0\n","12  0     0     0      0  1   0      0      0      0     0     0\n","13  0     0     0      0  0   0      0      1      0     0     0\n","14  0     1     0      0  0   0      0      0      0     0     0\n","15  0     0     0      0  0   0      1      0      0     0     0\n","16  0     0     1      0  0   0      0      0      0     0     0\n","17  0     0     0      0  0   0      0      0      0     0     0\n","18  0     0     0      0  0   0      0      0      0     0     0\n","19  0     0     0      0  0   0      0      0      0     0     0\n","20  0     0     0      0  0   0      0      0      0     0     0\n","21  0     0     0      0  0   0      0      0      0     0     0\n","22  0     0     0      0  0   0      0      0      0     1     0\n","23  0     0     0      0  0   0      1      0      0     0     0"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"UogvqSgPYdyI"},"source":["6.TF-IDF\n","\n","BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。\n","\n","\n","《標準的なTF-IDFの式》\n","\n","\n","Term Frequency:\n","\n","$$\n","tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}\n","$$\n","\n","$n_{t,d}$ : サンプルd内のトークンtの出現回数（BoWと同じ）\n","\n","\n","$\\sum_{s \\in d}n_{s,d}$ : サンプルdの全トークンの出現回数の和\n","\n","\n","Inverse Document Frequency:\n","\n","$$\n","idf(t) = \\log{\\frac{N}{df(t)}}\n","$$\n","\n","$N$ : サンプル数\n","\n","\n","$df(t)$ : トークンtが出現するサンプル数\n","\n","\n","＊logの底は任意の値\n","\n","\n","TF-IDF:\n","\n","$$\n","tfidf(t, d) = tf(t, d) \\times idf(t)\n","$$\n","\n","IDF\n","IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n","\n","\n","サンプル数 $N$ をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$ を変化させたグラフを確認してみると、次のようになります。\n","\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","n_samples = 25000\n","idf = np.log(n_samples/np.arange(1,n_samples))\n","plt.title(\"IDF\")\n","plt.xlabel(\"df(t)\")\n","plt.ylabel(\"IDF\")\n","plt.plot(idf)\n","plt.show()\n","\n","Image from Gyazo\n","\n","\n","TF-IDFではこの数を出現回数に掛け合わせるので、珍しいトークンの登場に重み付けを行なっていることになります。\n","\n","\n","ストップワード\n","あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを ストップワード と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n","\n","\n","scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。\n","\n","\n","vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n","bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n","df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n","display(df)\n","\n","代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。\n","\n","\n","\n","# はじめて使う場合はストップワードをダウンロード\n","import nltk\n","stop_words = nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ...\n","\n","逆に、登場回数が特に少ないトークンも取り除くことが多いです。すべてのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n","\n","\n","scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。\n","\n","\n","\n","vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n","bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n","df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n","display(df)"]},{"cell_type":"markdown","metadata":{"id":"gJvQIDQPYd3W"},"source":["【問題2】TF-IDFの計算\n","IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n","\n","\n","TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n","\n","\n","sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation\n","sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation\n","\n","\n","なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n","\n","\n","また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n","\n","\n","Term Frequency:\n","\n","\n","t\n","f\n","(\n","t\n",",\n","d\n",")\n","=\n","n\n","t\n",",\n","d\n","\n","$n_{t,d}$ : サンプルd内のトークンtの出現回数\n","\n","\n","scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n","\n","\n","Inverse Document Frequency:\n","\n","\n","i\n","d\n","f\n","(\n","t\n",")\n","=\n","log\n","1\n","+\n","N\n","1\n","+\n","d\n","f\n","(\n","t\n",")\n","+\n","1\n","\n","$N$ : サンプル数\n","\n","\n","$df(t)$ : トークンtが出現するサンプル数\n","\n","\n","＊logの底はネイピア数e\n","\n","\n","詳細は以下のドキュメントを確認してください。\n","\n","\n","5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473},"id":"Gd4NVC4laFW6","executionInfo":{"status":"ok","timestamp":1631177813509,"user_tz":-540,"elapsed":6681,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"803fdd09-776e-4ee7-dab2-5ec551fca694"},"source":["#CountVectorizerを使った場合\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk\n","stop_words = nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","nltk_stop_words = stopwords.words('english')\n","vectorizer = CountVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n","bow_train = (vectorizer.fit_transform(x_train)).toarray()\n","df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n","display(df)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>13th</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>1930</th>\n","      <th>1930s</th>\n","      <th>1933</th>\n","      <th>1940</th>\n","      <th>1950</th>\n","      <th>1950s</th>\n","      <th>1960</th>\n","      <th>1960s</th>\n","      <th>1968</th>\n","      <th>1970</th>\n","      <th>1970s</th>\n","      <th>1971</th>\n","      <th>1972</th>\n","      <th>1973</th>\n","      <th>1980</th>\n","      <th>1980s</th>\n","      <th>1983</th>\n","      <th>1984</th>\n","      <th>1987</th>\n","      <th>1990</th>\n","      <th>1993</th>\n","      <th>1995</th>\n","      <th>1996</th>\n","      <th>1997</th>\n","      <th>1999</th>\n","      <th>...</th>\n","      <th>worthwhile</th>\n","      <th>worthy</th>\n","      <th>would</th>\n","      <th>wound</th>\n","      <th>wounded</th>\n","      <th>wow</th>\n","      <th>wrap</th>\n","      <th>wrapped</th>\n","      <th>wreck</th>\n","      <th>wrestling</th>\n","      <th>write</th>\n","      <th>writer</th>\n","      <th>writers</th>\n","      <th>writes</th>\n","      <th>writing</th>\n","      <th>written</th>\n","      <th>wrong</th>\n","      <th>wrote</th>\n","      <th>wwii</th>\n","      <th>x</th>\n","      <th>ya</th>\n","      <th>yard</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>years</th>\n","      <th>yelling</th>\n","      <th>yellow</th>\n","      <th>yes</th>\n","      <th>yesterday</th>\n","      <th>yet</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>younger</th>\n","      <th>youth</th>\n","      <th>z</th>\n","      <th>zero</th>\n","      <th>zizek</th>\n","      <th>zombie</th>\n","      <th>zombies</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 5000 columns</p>\n","</div>"],"text/plain":["       0  00  000  1  10  100  11  ...  youth  z  zero  zizek  zombie  zombies  zone\n","0      0   0    0  0   0    0   0  ...      0  0     1      0       0        0     0\n","1      0   0    0  0   0    0   0  ...      0  0     0      0       0        0     0\n","2      0   0    0  0   1    0   0  ...      0  0     0      0       0        0     0\n","3      0   0    0  0   1    0   0  ...      0  0     0      0       0        0     0\n","4      0   0    0  0   0    0   0  ...      0  0     0      0       0        0     0\n","...   ..  ..  ... ..  ..  ...  ..  ...    ... ..   ...    ...     ...      ...   ...\n","24995  0   0    0  0   0    1   0  ...      0  0     0      0       0        0     0\n","24996  0   0    0  0   0    0   0  ...      0  0     0      0       0        0     0\n","24997  0   0    0  0   1    0   0  ...      0  0     0      0       0        0     0\n","24998  0   0    0  1   0    0   0  ...      0  0     0      0       0        0     0\n","24999  0   0    0  0   0    0   0  ...      0  0     0      0       0        0     0\n","\n","[25000 rows x 5000 columns]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"7ISvGtRSnHaJ","colab":{"base_uri":"https://localhost:8080/","height":473},"executionInfo":{"status":"ok","timestamp":1631177819718,"user_tz":-540,"elapsed":6213,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"954d6910-6d60-47ce-bac9-d287e80baffb"},"source":["#TfidfVectorizerを使った場合\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","stop_words = nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","nltk_stop_words = stopwords.words('english')\n","vectorizer = TfidfVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n","bow_train = (vectorizer.fit_transform(x_train)).toarray()\n","df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n","display(df)"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>13th</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>1930</th>\n","      <th>1930s</th>\n","      <th>1933</th>\n","      <th>1940</th>\n","      <th>1950</th>\n","      <th>1950s</th>\n","      <th>1960</th>\n","      <th>1960s</th>\n","      <th>1968</th>\n","      <th>1970</th>\n","      <th>1970s</th>\n","      <th>1971</th>\n","      <th>1972</th>\n","      <th>1973</th>\n","      <th>1980</th>\n","      <th>1980s</th>\n","      <th>1983</th>\n","      <th>1984</th>\n","      <th>1987</th>\n","      <th>1990</th>\n","      <th>1993</th>\n","      <th>1995</th>\n","      <th>1996</th>\n","      <th>1997</th>\n","      <th>1999</th>\n","      <th>...</th>\n","      <th>worthwhile</th>\n","      <th>worthy</th>\n","      <th>would</th>\n","      <th>wound</th>\n","      <th>wounded</th>\n","      <th>wow</th>\n","      <th>wrap</th>\n","      <th>wrapped</th>\n","      <th>wreck</th>\n","      <th>wrestling</th>\n","      <th>write</th>\n","      <th>writer</th>\n","      <th>writers</th>\n","      <th>writes</th>\n","      <th>writing</th>\n","      <th>written</th>\n","      <th>wrong</th>\n","      <th>wrote</th>\n","      <th>wwii</th>\n","      <th>x</th>\n","      <th>ya</th>\n","      <th>yard</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>years</th>\n","      <th>yelling</th>\n","      <th>yellow</th>\n","      <th>yes</th>\n","      <th>yesterday</th>\n","      <th>yet</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>younger</th>\n","      <th>youth</th>\n","      <th>z</th>\n","      <th>zero</th>\n","      <th>zizek</th>\n","      <th>zombie</th>\n","      <th>zombies</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.054256</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.164505</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.136932</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.103928</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.124442</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.085006</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.104143</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.052273</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.427149</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.064930</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.03913</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.036232</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.095763</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.093618</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.075540</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.068246</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.101521</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.059395</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.095056</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.050623</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 5000 columns</p>\n","</div>"],"text/plain":["         0   00  000         1  ...  zizek  zombie  zombies  zone\n","0      0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","1      0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","2      0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","3      0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","4      0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","...    ...  ...  ...       ...  ...    ...     ...      ...   ...\n","24995  0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","24996  0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","24997  0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","24998  0.0  0.0  0.0  0.101521  ...    0.0     0.0      0.0   0.0\n","24999  0.0  0.0  0.0  0.000000  ...    0.0     0.0      0.0   0.0\n","\n","[25000 rows x 5000 columns]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"bmBL-lf9nHeP","executionInfo":{"status":"ok","timestamp":1631180786706,"user_tz":-540,"elapsed":222,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":[""],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SEqawJEvYd8i"},"source":["【問題3】TF-IDFを用いた学習\n","問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n","\n","\n","ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。\n","\n"]},{"cell_type":"code","metadata":{"id":"-zncNMmNaFxO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631181661748,"user_tz":-540,"elapsed":29050,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"65d1da08-c720-4e2b-a014-7039c558dc7e"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from matplotlib import pyplot as plt\n","from matplotlib.colors import Normalize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","# stop_words = nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","\n","nltk_stop_words = stopwords.words('english')\n","vectorizer = TfidfVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n","bow_train = (vectorizer.fit_transform(x_train)).toarray()\n","bow_test = (vectorizer.fit_transform(x_test)).toarray()\n","# データが変換されると、学習や予測に利用できるようになります。\n","\n","# ロジスティック回帰による学習\n","clf = LogisticRegression(C=1.0, solver='lbfgs')\n","clf.fit(bow_train, y_train)\n","r2 = clf.score(bow_train, y_train)\n","Z = clf.predict(bow_test)\n","print(Z)\n"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 1 ... 1 1 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"zVtWuulVYeCw"},"source":["【問題4】TF-IDFのスクラッチ実装\n","以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n","\n","\n","This movie is SOOOO funny!!!\n","\n","What a movie! I never\n","\n","best movie ever!!!!! this movie"]},{"cell_type":"code","metadata":{"id":"4wsxJESgaORp","executionInfo":{"status":"ok","timestamp":1631181899636,"user_tz":-540,"elapsed":235,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":["#TF-IDFスクラッチ\n","import numpy as np\n","import re\n","\n","class TF_IDF():\n","\n","    def __init__(self, corpus):\n","        self.corpus = corpus\n","\n","    def tf(self):\n","        l = []\n","        c = []\n","\n","    #corpusの各テキストを単語毎に分割。\n","    #sklearnの結果と合わせるため、分割方法は正規表現でのマッチで行う\n","\n","        for text in self.corpus:\n","            c +=  re.findall(r'\\b\\w+\\b', text)\n","\n","    #抽出した単語の重複を削除\n","        c = list(set(c))\n","    #各テキスト毎の単語出現回数をカウントし、該当テキストの総単語数で割る\n","\n","        for text in self.corpus:\n","            xxx = re.findall(r'\\b\\w+\\b', text)\n","            # l.append([xxx.count(i)/len(xxx) for i in c])\n","            # l.append([xxx.count(i)/len(xxx) for i in c])\n","            l.append([xxx.count(i)/len(xxx) if len(xxx)>0 else 1 for i in c])\n","\n","        return np.array(l)\n","\n","    def idf(self):\n","\n","        terms = []\n","\n","    #corpusの各テキストを単語毎に分割。\n","    #sklearの結果と合わせるため、分割方法は正規表現でのマッチで行う\n","\n","        for text in self.corpus:\n","            terms +=  re.findall(r'\\b\\w+\\b', text)\n","\n","        terms = list(set(terms))\n","\n","        l = []\n","\n","        for term in terms:\n","            #各単語がそのテキストに含まれているかどうかをカウント\n","            c = 0\n","\n","            for text in self.corpus:\n","                #各テキストを単語単位に分割\n","                word_list = re.findall(r'\\b\\w+\\b', text)\n","                #該当テキスト内に含まれている単語であれば、１カウントする\n","                #重複カウントを防ぐ為に論理演算子は「in」を用いる\n","                #文章の繋がりで、単語ではないものを単語としてカウントしないように上記でリスト化している\n","                if term in word_list:\n","                    c += 1\n","\n","                #各単語IDFを計算。sklearnの計算と合わせるため、分母分子に１を足し、更にその計算結果にも1を足す。\n","            l.append(np.log((1 + len(self.corpus))/(c+1)) + 1) \n","\n","        return np.array(l)    \n","\n","    # def l2(self, x):\n","    #     #l2ノルムで正規化する（単位ベクトル化する）\n","    #     l2 = x / np.sqrt(np.sum(x**2))\n","\n","    #     return l2\n","\n","    def tf_idf(self):\n","\n","        xxxx = self.tf()*self.idf()\n","        return np.array(xxxx)\n","        #各行にl2ノルムの正規化を適用\n","        # return np.array([self.l2(a) for a in xxxx])\n"],"execution_count":74,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RO8nx9G2rhIo","executionInfo":{"status":"ok","timestamp":1631177828717,"user_tz":-540,"elapsed":9004,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"fafabeaf-290e-4e6b-d4f5-ce405ebeec14"},"source":["# !pip install mecab-python3\n","# !pip install unidic"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mecab-python3\n","  Downloading mecab_python3-1.0.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (488 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 488 kB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.4\n","Collecting unidic\n","  Downloading unidic-1.0.3.tar.gz (5.1 kB)\n","Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.7/dist-packages (from unidic) (4.62.0)\n","Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic) (0.8.2)\n","Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic) (1.1.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.22.0->unidic) (1.24.3)\n","Building wheels for collected packages: unidic\n","  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic: filename=unidic-1.0.3-py3-none-any.whl size=5506 sha256=501c4bd864f3d8867329716d61b23e75605ad75bdbed7b494647e4aafd86d951\n","  Stored in directory: /root/.cache/pip/wheels/23/30/0b/128289fb595ef4117d2976ffdbef5069ef83be813e88caa0a6\n","Successfully built unidic\n","Installing collected packages: unidic\n","Successfully installed unidic-1.0.3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3A3plBas6zm","executionInfo":{"status":"ok","timestamp":1631177875388,"user_tz":-540,"elapsed":46675,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"2888ad87-44ee-4a67-fe12-b0f3042d247b"},"source":["# !python -m unidic download"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["download url: https://cotonoha-dic.s3-ap-northeast-1.amazonaws.com/unidic.zip\n","Dictionary version: 2.3.0+2020-10-08\n","Downloading UniDic v2.3.0+2020-10-08...\n","unidic.zip: 100% 608M/608M [00:35<00:00, 17.0MB/s]\n","Finished download.\n","Downloaded UniDic v2.3.0+2020-10-08 to /usr/local/lib/python3.7/dist-packages/unidic/dicdir\n"]}]},{"cell_type":"code","metadata":{"id":"uw0J5ej4pU5a","executionInfo":{"status":"ok","timestamp":1631181783767,"user_tz":-540,"elapsed":248,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":["import MeCab\n"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"jn8oLy7FryQ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631181905923,"user_tz":-540,"elapsed":223,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"aa4c7c3d-e926-46f8-f785-c056b9bd96e5"},"source":["wakati = MeCab.Tagger(\"-Owakati\")\n","words = wakati.parse(\"This movie is SOOOO funny!!! What a movie! I never best movie ever!!!!! this movie\").split()\n","print(words)"],"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'movie', 'is', 'SOOOO', 'funny', '!', '!', '!', 'What', 'a', 'movie', '!', 'I', 'never', 'best', 'movie', 'ever', '!', '!', '!', '!', '!', 'this', 'movie']\n"]}]},{"cell_type":"code","metadata":{"id":"Dni32bKqpU9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631181907468,"user_tz":-540,"elapsed":208,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"d9791c21-76a1-4341-c55b-f0cf731909b7"},"source":["#自力実装\n","import numpy as np\n","import re\n","# from module.tf_idf import TF_IDF\n","\n","x = TF_IDF(words)\n","tf_idf_a =x.tf_idf()\n","\n","print(np.sum(tf_idf_a[0]))"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["3.5257286443082556\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYSfDYEfpVBh","executionInfo":{"status":"ok","timestamp":1631177875390,"user_tz":-540,"elapsed":12,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"62438104-f097-4de2-c8b4-ab74f90361e3"},"source":["#sklearnで導出\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", lowercase = False,stop_words = None)\n","\n","X = vectorizer.fit_transform(words)\n","\n","np.sum(X.toarray()[0])"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"40QXeRAgpVfD","executionInfo":{"status":"ok","timestamp":1631177875390,"user_tz":-540,"elapsed":9,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":[""],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oOAXA9gYYeMK"},"source":["7.Word2Vec\n","\n","ニューラルネットワークを用いてベクトル化を行う手法が Word2Vec です。\n","\n","\n","BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを Word Embedding（単語埋め込み） や 分散表現 と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n","\n","\n","Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。\n","\n","\n","CBoW\n","CBoW (Continuous Bag-of-Words) によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n","\n","\n","単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n","\n","\n","間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n","\n","\n","あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える パディング を行なったり、長いテキストは単語を消したりします。テキストを 固定長 にすると呼びます。\n","\n","\n","ウィンドウサイズ\n","入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを ウィンドウサイズ と呼びます。\n","\n","\n","Skip-gram\n","CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が Skip-gram です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。\n","\n","\n","利用方法\n","Pythonでは Gensim ライブラリを用いて扱うことができます。\n","\n","\n","gensim: models.word2vec – Word2vec embeddings\n","\n","\n","BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n","\n","\n","デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","from gensim.models import Word2Vec\n","sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n","model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n","model.build_vocab(sentences) # 準備\n","model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n","\n","print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n","for vocab in model.wv.vocab.keys():\n","  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))\n","\n","このようにしてベクトルが得られます。\n","\n","\n","単語の距離\n","ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。\n","\n","\n","1\n","model.wv.most_similar(positive=\"good\", topn=3)\n","\n","今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。\n","\n","\n","可視化\n","2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","vocabs = model.wv.vocab.keys()\n","tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n","vectors_tsne = tsne_model.fit_transform(model[vocabs])\n","fig, ax = plt.subplots(figsize=(5,5))\n","ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n","for i, word in enumerate(list(vocabs)):\n","    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n","ax.set_yticklabels([])\n","ax.set_xticklabels([])\n","plt.show()\n","\n","8.IMDB映画レビューデータセットの分散表現\n","\n","IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。\n","\n"]},{"cell_type":"code","metadata":{"id":"bMFdid2aaUB7","executionInfo":{"status":"ok","timestamp":1631177875391,"user_tz":-540,"elapsed":9,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":[""],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_ik6jYyaUl4"},"source":["【問題5】コーパスの前処理\n","コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"]},{"cell_type":"code","metadata":{"id":"iNfeNTrQaZE8","executionInfo":{"status":"ok","timestamp":1631177875391,"user_tz":-540,"elapsed":9,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":["import re\n","def CorpusPrepro(corpus):\n","  text = corpus\n","  text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text) #URL削除\n","  text = text.lower() #大文字の小文字化\n","  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text) #特殊文字削除\n","  text.split()\n","  print(text)\n","  return text"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34oo8k31aUv9"},"source":["【問題6】Word2Vecの学習\n","Word2Vecの学習を行なってください。"]},{"cell_type":"code","metadata":{"id":"y_RObzTGac2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631177878664,"user_tz":-540,"elapsed":3282,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"51f1ebbd-6797-4e6f-8c8e-f5c4593c9e62"},"source":["!pip install gensim"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avkPA1PCBKuQ","executionInfo":{"status":"ok","timestamp":1631177879425,"user_tz":-540,"elapsed":768,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}},"outputId":"6aa651a4-03e0-4ba9-cc97-46c1a730e96d"},"source":["from gensim.models import word2vec\n","corpus = x_train[0]\n","laernData = CorpusPrepro(corpus)\n","\n","model = word2vec.Word2Vec(laernData, size=100, min_count=5, window=5, iter=3)\n","model.save(\"IMDB_review.model\")"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["zero day leads you to think even rethink why two boysyoung men would do what they did  commit mutual suicide via slaughtering their classmates it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their ownmutual world via coupled destructionbr br it is not a perfect movie but given what moneytime the filmmaker and actors had  it is a remarkable product in terms of explaining the motives and actions of the two young suicidemurderers it is better than 'elephant'  in terms of being a film that gets under our 'rationalistic' skin it is a far far better film than almost anything you are likely to see br br flawed but honest with a terrible honesty\n"]}]},{"cell_type":"code","metadata":{"id":"y1_wMlzhBKy6","executionInfo":{"status":"ok","timestamp":1631177879426,"user_tz":-540,"elapsed":8,"user":{"displayName":"木村剛","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13118920826758032389"}}},"source":[""],"execution_count":46,"outputs":[]}]}