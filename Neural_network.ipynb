{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural_network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5expfE4kHZglUSzo9Use7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iO_Yv4-KahZT"},"source":["Sprintの目的\n","スクラッチを通してニューラルネットワークの基礎を理解する\n","画像データの扱い方を知る"]},{"cell_type":"markdown","metadata":{"id":"sJyd2qPjauG7"},"source":["ニューラルネットワークスクラッチの検証にはMNISTデータセットを使用します。各種ライブラリやサイトからダウンロードできますが、ここでは深層学習フレームワークのKerasを用います。以下のコードを実行すればデータセットをダウンロードし、展開まで行えます。\n","\n","\n","《データセットをダウンロードするコード》\n","\n","\n","1\n","2\n","from keras.datasets import mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"8vRsJqFGa1HM"},"source":["《MNISTとは？》\n","\n","\n","画像分類のための定番データセットで、手書き数字認識を行います。このデータセットには訓練用6万枚、テスト用1万枚の28×28ピクセルの白黒画像、およびそれらが0〜9のどの数字であるかというラベルが含まれています。\n","\n","\n","《画像データとは？》\n","\n","\n","デジタル画像は点の集合で、これをピクセルと呼びます。一般的に白黒画像であればピクセルには0〜255の値が含まれます。一方、カラー画像であればR（赤）、G（緑）、B（青）それぞれに対応する0〜255の値が含まれます。機械学習をする上では、この0〜255の値一つひとつが特徴量として扱われます。0〜255は符号なしの8ビット整数で表せる範囲になるため、NumPyであれば「uint8」型の変数として保持できます。\n","\n","\n","データセットの確認\n","どういったデータなのかを見てみます。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","4\n","print(X_train.shape) # (60000, 28, 28)\n","print(X_test.shape) # (10000, 28, 28)\n","print(X_train[0].dtype) # uint8\n","print(X_train[0])\n","\n","各データは28×28ピクセルの白黒画像です。\n","\n","\n","平滑化\n","(1, 28, 28)の各画像を、(1, 784)に変換します。これまで学んできた機械学習手法や、今回扱う全結合層のみのニューラルネットワークではこの形で扱います。すべてのピクセルが一列になっていることを、 平滑化（flatten） してあるという風に表現します。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","\n","《補足》\n","\n","\n","ここまで機械学習を学んでくる中で、特徴量の数を「次元」と呼んできました。その視点ではMNISTは784次元のデータです。一方で、NumPyのshapeが(784,)の状態を1次元配列とも呼びます。画像としての縦横の情報を持つ（28, 28)の状態であれば、2次元配列です。この視点では2次元のデータです。さらに、もしカラー画像であれば(28, 28, 3)ということになり、3次元配列です。先ほどの視点では3次元のデータになります。しかし、白黒でもカラーでも平面画像であり、立体データではないという視点で、2次元のデータです。画像データを扱う際にはこのように「次元」という言葉が複数の意味合いで使われることに注意してください。\n","\n","\n","画像データの可視化\n","画像データを可視化します。plt.imshowに渡します。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","index = 0\n","image = X_train[index].reshape(28,28)\n","# X_train[index]: (784,)\n","# image: (28, 28)\n","plt.imshow(image, 'gray')\n","plt.title('label : {}'.format(y_train[index]))\n","plt.show()\n","\n","numpy.reshape — NumPy v1.17 Manual\n","\n","\n","matplotlib.pyplot.imshow — Matplotlib 3.1.1 documentation\n","\n","\n","《発展的話題》\n","\n","\n","画像データは符号なし8ビット整数のuint8型で保持されることが一般的ですが、plt.imshowはより自由な配列を画像として表示することが可能です。例えば、以下のようにマイナスの値を持ったfloat64型の浮動小数点であってもエラーにはならないし、先ほどとまったく同じ風に表示されます。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","index = 0\n","image = X_train[index].reshape(28,28)\n","image = image.astype(np.float) # float型に変換\n","image -= 105.35 # 意図的に負の小数値を作り出してみる\n","plt.imshow(image, 'gray')\n","plt.title('label : {}'.format(y_train[index]))\n","plt.show()\n","print(image) # 値を確認\n","\n","これは、自動的に値を0〜255の整数に変換して処理するように作られているからです。uint8型であっても最小値が0、最大値が255でない場合には色合いがおかしくなります。それを防ぐためには次のように引数を入れてください。\n","\n","\n","1\n","plt.imshow(image, 'gray', vmin = 0, vmax = 255)\n","\n","画像関係のライブラリではこの自動的なスケーリングが思わぬ結果を生むことがあるので、新しいメソッドを使うときには確認しておきましょう。\n","\n","\n","前処理\n","画像は0から255のuint8型で表されますが、機械学習をする上では0から1のfloat型で扱うことになります。以下のコードで変換可能です。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","X_train = X_train.astype(np.float)\n","X_test = X_test.astype(np.float)\n","X_train /= 255\n","X_test /= 255\n","print(X_train.max()) # 1.0\n","print(X_train.min()) # 0.0\n","\n","また、正解ラベルは0から9の整数ですが、ニューラルネットワークで多クラス分類を行う際には one-hot表現 に変換します。scikit-learnのOneHotEncoderを使用したコードが以下です。このone-hot表現による値はそのラベルである確率を示していることになるため、float型で扱います。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n","y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n","print(y_train.shape) # (60000,)\n","print(y_train_one_hot.shape) # (60000, 10)\n","print(y_train_one_hot.dtype) # float64\n","\n","sklearn.preprocessing.OneHotEncoder — scikit-learn 0.21.3 documentation\n","\n","\n","さらに、訓練データ6万枚の内2割を検証データとして分割してください。訓練データが48000枚、検証データが12000枚となります。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","print(X_train.shape) # (48000, 784)\n","print(X_val.shape) # (12000, 784)\n","\n","3.ニューラルネットワークスクラッチ\n","\n","ニューラルネットワークのクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n","\n","\n","今回は多クラス分類を行う3層のニューラルネットワークを作成します。層の数などは固定した上でニューラルネットワークの基本を学びます。次のSprintで層を自由に変えられる設計にしていきます。\n","\n","\n","以下に雛形を用意してあります。このScratchSimpleNeuralNetrowkClassifierクラスにコードを書き加えていってください。\n","\n","\n","《雛形》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","class ScratchSimpleNeuralNetrowkClassifier():\n","    \"\"\"\n","    シンプルな三層ニューラルネットワーク分類器\n","    Parameters\n","    ----------\n","    Attributes\n","    ----------\n","    \"\"\"\n","    def __init__(self, verbose = True):\n","        self.verbose = verbose\n","        pass\n","    def fit(self, X, y, X_val=None, y_val=None):\n","        \"\"\"\n","        ニューラルネットワーク分類器を学習する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            訓練データの特徴量\n","        y : 次の形のndarray, shape (n_samples, )\n","            訓練データの正解値\n","        X_val : 次の形のndarray, shape (n_samples, n_features)\n","            検証データの特徴量\n","        y_val : 次の形のndarray, shape (n_samples, )\n","            検証データの正解値\n","        \"\"\"\n","        if self.verbose:\n","            #verboseをTrueにした際は学習過程などを出力する\n","            print()\n","        pass\n","    def predict(self, X):\n","        \"\"\"\n","        ニューラルネットワーク分類器を使い推定する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","        Returns\n","        -------\n","            次の形のndarray, shape (n_samples, 1)\n","            推定結果\n","        \"\"\"\n","        pass\n","        return\n","\n","ミニバッチ処理\n","これまでの機械学習スクラッチでは、すべてのサンプルを一度に計算していました。しかし、ニューラルネットワークではデータを分割して入力する 確率的勾配降下法 が一般的です。分割した際のひとかたまりを ミニバッチ 、そのサンプル数を バッチサイズ と呼びます。\n","\n","\n","今回はバッチサイズを20とします。今回使う訓練データは48000枚ですから、48000÷20で2400回の更新を繰り返すことになります。ニューラルネットワークではこれを2400回 イテレーション（iteration） すると呼びます。訓練データを一度すべて見ると1回の エポック（epoch） が終わったことになります。このエポックを複数回繰り返し、学習が完了します。\n","\n","\n","これを実現するための簡素なイテレータを用意しました。for文で呼び出すと、ミニバッチを取得できます。\n","\n","\n","《コード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 20, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self._X = X[shuffle_index]\n","        self._y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self._X[p0:p1], self._y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self._X[p0:p1], self._y[p0:p1]\n","\n","このクラスをインスタンス化し、for文を使うことでミニバッチが取り出せます。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n","print(len(get_mini_batch)) # 2400\n","print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n","for mini_X_train, mini_y_train in get_mini_batch:\n","    # このfor文内でミニバッチが使える\n","    pass\n","\n","__getitem__や__next__は__init__と同じ特殊メソッドの一種です。\n","\n","\n","学習\n","ニューラルネットワークの学習はフォワードプロパゲーションとバックプロパゲションの繰り返しになります。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gvJ-_tm0a-n5"},"source":["【問題1】重みの初期値を決めるコードの作成\n","ニューラルネットワークの各層の重みの初期値を決めるコードを作成してください。\n","\n","\n","重みの初期値はさまざまな方法が提案されていますが、今回はガウス分布による単純な初期化を行います。バイアスに関しても同様です。\n","\n","\n","以下のコードを参考にしてください。標準偏差の値sigmaはハイパーパラメータです。発展的な重みの初期化方法については次のSprintで扱います。\n","\n","\n","《サンプルコード》\n","\n","\n","1\n","2\n","3\n","4\n","5\n","n_features = 784\n","n_nodes1 = 400\n","sigma = 0.01 # ガウス分布の標準偏差\n","W1 = sigma * np.random.randn(n_features, n_nodes1)\n","# W1: (784, 400)\n","\n","numpy.random.randn — NumPy v1.15 Manual\n","\n"]},{"cell_type":"code","metadata":{"id":"SOV8DhptiZvV"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6GCye01rjKND"},"source":["画像関係のライブラリではこの自動的なスケーリングが思わぬ結果を生むことがあるので、新しいメソッドを使うときには確認しておきましょう。\n","\n","前処理 画像は0から255のuint8型で表されますが、機械学習をする上では0から1のfloat型で扱うことになります。 以下のコードで変換可能です。\n","\n","《サンプルコード》"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FSEsSMZdajVU","executionInfo":{"status":"ok","timestamp":1629295636623,"user_tz":-540,"elapsed":2415,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"a5068f50-6ac5-4c78-ea5f-9f1d93a3a1a6"},"source":["import numpy as np\n","from keras.datasets import mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","\n","print(X_train.shape) # (60000, 28, 28)\n","print(X_test.shape) # (10000, 28, 28)\n","print(X_train[0].dtype) # uint8\n","print(X_train[0])\n","\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","index = 0\n","image = X_train[index].reshape(28,28)\n","# X_train[index]: (784,)\n","# image: (28, 28)\n","plt.imshow(image, 'gray')\n","plt.title('label : {}'.format(y_train[index]))\n","plt.show()\n","\n","index = 0\n","image = X_train[index].reshape(28,28)\n","image = image.astype(np.float) # float型に変換\n","image -= 105.35 # 意図的に負の小数値を作り出してみる\n","plt.imshow(image, 'gray')\n","plt.title('label : {}'.format(y_train[index]))\n","plt.show()\n","print(image) # 値を確認\n","\n","plt.imshow(image, 'gray', vmin = 0, vmax = 255)\n","\n","\n","X_train = X_train.astype(np.float)\n","X_test = X_test.astype(np.float)\n","X_train /= 255\n","X_test /= 255\n","print(X_train.max()) # 1.0\n","print(X_train.min()) # 0.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(60000, 28, 28)\n","(10000, 28, 28)\n","uint8\n","[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n","  175  26 166 255 247 127   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n","  225 172 253 242 195  64   0   0   0   0]\n"," [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n","   93  82  82  56  39   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n","   25   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n","  150  27   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n","  253 187   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n","  253 249  64   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n","  253 207   2   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n","  250 182   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n","   78   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n","    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n","   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n","   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n","   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n","   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n","   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n","    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n","   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n","    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n","   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n","   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n","   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n","   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n","   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n","   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n","   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n","   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n","    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]\n"," [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n","  -105.35]]\n","1.0\n","0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Srp-t-ojUYL","executionInfo":{"status":"ok","timestamp":1629295636993,"user_tz":-540,"elapsed":11,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"63b76398-8e2c-4b63-ba89-d8da77e3267c"},"source":["from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n","y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n","print(y_train.shape) # (60000,)\n","print(y_train_one_hot.shape) # (60000, 10)\n","print(y_train_one_hot.dtype) # float64"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(60000,)\n","(60000, 10)\n","float64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gnmupSDOUfOU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629295730743,"user_tz":-540,"elapsed":220,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"4cd7cc5b-607c-425c-ae16-dc5e7e356bd2"},"source":["X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n","print(X_train.shape) # (48000, 784)\n","print(X_val.shape) # (12000, 784)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(48000, 784)\n","(12000, 784)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IplIuzQul_Gl"},"source":["class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 20, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self._X = X[shuffle_index]\n","        self._y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self._X[p0:p1], self._y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self._X[p0:p1], self._y[p0:p1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVry8Z6ymOUf"},"source":["このクラスをインスタンス化し、for文を使うことでミニバッチが取り出せます。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8xtjRMRmRNa","executionInfo":{"status":"ok","timestamp":1629295822949,"user_tz":-540,"elapsed":594,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"e96d04a7-b0f8-4400-f412-571991b92213"},"source":["get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n","print(len(get_mini_batch)) # 2400\n","print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n","for mini_X_train, mini_y_train in get_mini_batch:\n","    # このfor文内でミニバッチが使える\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2400\n","(array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]]), array([2, 9, 9, 4, 7, 8, 6, 9, 5, 9, 0, 4, 0, 1, 2, 7, 9, 0, 5, 8],\n","      dtype=uint8))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZJgJVxuemglv"},"source":["n_features = 784\n","n_nodes1 = 400\n","sigma = 0.01 # ガウス分布の標準偏差\n","W1 = sigma * np.random.randn(n_features, n_nodes1)\n","# W1: (784, 400)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u1Y8DpnnmsFs"},"source":["def __init__(self, verbose = True):\n","    self.verbose = verbose\n","\n","    self.batch_size = 20 # バッチサイズ\n","    self.n_features = 784 # 特徴量の数\n","    self.n_nodes1 = 400 # 1層目のノード数\n","    self.n_nodes2 = 200 # 2層目のノード数\n","    self.n_output = 10 # 出力のクラス数（3層目のノード数）\n","\n","    self.sigma = 0.01 # ガウス分布の標準偏差\n","\n","    self.network = {} \n","    self.network['W1'] = sigma * np.random.randn(n_features, n_nodes1)\n","    self.network['W2'] = sigma * np.random.randn(n_nodes1, n_nodes2)        \n","    self.network['W3'] = sigma * np.random.randn(n_nodes2, n_output)    \n","    \n","    # Xavierの初期値\n","    # network['W1'] = np.random.rand(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size)\n","    # network['W2'] = np.random.rand(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size)\n","    \n","    # Heの初期値\n","    # network['W1'] = np.random.rand(input_layer_size, hidden_layer_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n","    # network['W2'] = np.random.rand(hidden_layer_size, output_layer_size) / np.sqrt(hidden_layer_size) * np.sqrt(2)\n","\n","    network['b1'] = np.zeros(n_nodes1)\n","    network['b2'] = np.zeros(n_nodes2)\n","    network['b3'] = np.zeros(output_layer_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXfEMhCvahgk"},"source":["【問題2】フォワードプロパゲーションの実装\n","三層のニューラルネットワークの フォワードプロパゲーション を作成してください。以下の説明ではノード数は1層目は400、2層目は200としますが、変更しても構いません。\n","\n","\n","各層の数式を以下に示します。今回はそれぞれの記号が表す配列が、実装上どのようなndarrayのshapeになるかを併記してあります。\n","\n","\n","batch_size = 20 # バッチサイズ\n","n_features = 784 # 特徴量の数\n","n_nodes1 = 400 # 1層目のノード数\n","n_nodes2 = 200 # 2層目のノード数\n","n_output = 10 # 出力のクラス数（3層目のノード数）\n","\n","「1層目」\n","\n","\n","A\n","1\n","=\n","X\n","⋅\n","W\n","1\n","+\n","B\n","1\n","\n","$X$ : 特徴量ベクトル (batch_size, n_features)\n","\n","\n","$W_1$ : 1層目の重み (n_features, n_nodes1)\n","\n","\n","$B_1$ : 1層目のバイアス (n_nodes1,)\n","\n","\n","$A_1$ : 出力 (batch_size, n_nodes1)\n","\n","\n","「1層目の活性化関数」\n","\n","\n","Z\n","1\n","=\n","f\n","(\n","A\n","1\n",")\n","\n","$f()$ : 活性化関数\n","\n","\n","$Z_1$ 出力 (batch_size, n_nodes1)\n","\n","\n","「2層目」\n","\n","\n","A\n","2\n","=\n","Z\n","1\n","⋅\n","W\n","2\n","+\n","B\n","2\n","\n","$W_2$ : 2層目の重み (n_nodes1, n_nodes2)\n","\n","\n","$B_2$ : 2層目のバイアス (n_nodes2,)\n","\n","\n","$A_2$ : 出力 (batch_size, n_nodes2)\n","\n","\n","「2層目の活性化関数」\n","\n","\n","Z\n","2\n","=\n","f\n","(\n","A\n","2\n",")\n","\n","$f()$ : 活性化関数\n","\n","\n","$Z_2$ 出力 (batch_size, n_nodes2)\n","\n","\n","「3層目（出力層）」\n","\n","\n","A\n","3\n","=\n","Z\n","2\n","⋅\n","W\n","3\n","+\n","B\n","3\n","\n","$W_3$ : 3層目の重み (n_nodes2, n_output)\n","\n","\n","$B_3$ : 3層目のバイアス (n_output,)\n","\n","\n","$A_3$ : 出力 (batch_size, n_output)\n","\n","\n","「3層目の活性化関数」\n","\n","\n","Z\n","3\n","=\n","s\n","o\n","f\n","t\n","m\n","a\n","x\n","(\n","A\n","3\n",")\n","\n","$softmax()$ : ソフトマックス関数\n","\n","\n","$Z_3$ 出力 (batch_size, n_output)\n","\n","\n","$Z_3$ は各ラベル（0〜9）に対する確率の配列である。\n","\n","\n","活性化関数（フォワードプロバゲーション）\n","活性化関数を作成し、フォワードプロパゲーションの中で使用します。切り替えられるように実装することを推奨しますが、片方でも構いません。\n","\n","\n","「シグモイド関数」\n","\n","\n","f\n","(\n","Z\n",")\n","=\n","s\n","i\n","g\n","m\n","o\n","i\n","d\n","(\n","A\n",")\n","=\n","1\n","1\n","+\n","e\n","x\n","p\n","(\n","−\n","A\n",")\n","\n","指数関数 $exp(-A)$ の計算はnp.expを使用してください。\n","\n","\n","numpy.exp — NumPy v1.15 Manual\n","\n","\n","「ハイパボリックタンジェント関数」\n","\n","\n","次の数式で表されますが、np.tanhひとつで実現できます。\n","\n","\n","f\n","(\n","Z\n",")\n","=\n","t\n","a\n","n\n","h\n","(\n","A\n",")\n","=\n","e\n","x\n","p\n","(\n","A\n",")\n","−\n","e\n","x\n","p\n","(\n","−\n","A\n",")\n","e\n","x\n","p\n","(\n","A\n",")\n","+\n","e\n","x\n","p\n","(\n","−\n","A\n",")\n","\n","numpy.tanh — NumPy v1.15 Manual\n","\n","\n","＊現在ではこれらの代わりにReLUと呼ばれる活性化関数が一般的です。次のSprintで扱います。\n","\n","\n","ソフトマックス関数\n","ソフトマックス関数を作成し、フォワードプロパゲーションの中で使用します。これも活性化関数の一種ですが、多クラス分類の出力層で使われる特性上、区別して扱われることが多いです。\n","\n","\n","次の数式です。\n","\n","\n","Z\n","3\n","_\n","k\n","=\n","e\n","x\n","p\n","(\n","A\n","3\n","_\n","k\n",")\n","∑\n","n\n","c\n","i\n","=\n","1\n","e\n","x\n","p\n","(\n","A\n","3\n","_\n","i\n",")\n","\n","$Z_{3_k}$ : $k$ 番目のクラスの確率ベクトル (batch_size,)\n","\n","\n","$A_{3_k}$ : $k$ 番目のクラスにあたる前の層からのベクトル (batch_size,)\n","\n","\n","$n_c$ : クラスの数、n_output。今回のMNISTでは10。\n","\n","\n","分母はすべてのクラスに相当する値を指数関数に通した上で足し合わせたものです。その中で、分子に $k$ 番目のクラスを持ってくることで、 $k$ 番目のクラスである確率が求まります。\n","\n","\n","これを10クラス分計算し、合わせたものが $Z_3$ です。\n","\n"]},{"cell_type":"code","metadata":{"id":"XSkG8pD6nyUT"},"source":["class functions():\n","    # 中間層の活性化関数\n","    # シグモイド関数（ロジスティック関数）\n","    def sigmoid(self, x):\n","        return 1/(1 + np.exp(-x))\n","\n","    # ReLU関数\n","    def relu(self, x):\n","        return np.maximum(0, x)\n","\n","    # ステップ関数（閾値0）\n","    def step_function(self, x):\n","        return np.where( x > 0, 1, 0) \n","\n","    # 出力層の活性化関数\n","    # ソフトマックス関数\n","    def softmax(self, x):\n","        if x.ndim == 2:\n","            x = x.T\n","            x = x - np.max(x, axis=0)\n","            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","            return y.T\n","\n","        x = x - np.max(x) # オーバーフロー対策\n","        return np.exp(x) / np.sum(np.exp(x))\n","\n","    # ソフトマックスとクロスエントロピーの複合関数\n","    def softmax_with_loss(self, d, x):\n","        y = softmax(x)\n","        return cross_entropy_error(d, y)\n","\n","    # 誤差関数\n","    # 最小二乗法\n","    def least_square(self, d, y):\n","        return np.sum(np.square(d - y)) / 2\n","\n","    # クロスエントロピー\n","    def cross_entropy_error(self, d, y):\n","        if y.ndim == 1:\n","            d = d.reshape(1, d.size)\n","            y = y.reshape(1, y.size)\n","\n","        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","        if d.size == y.size:\n","            d = d.argmax(axis=1)\n","\n","        batch_size = y.shape[0]\n","        return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n","\n","\n","\n","    # 活性化関数の導関数\n","    # シグモイド関数（ロジスティック関数）の導関数\n","    def d_sigmoid(self, x):\n","        dx = (1.0 - self.sigmoid(x)) * self.sigmoid(x)\n","        return dx\n","\n","    # ReLU関数の導関数\n","    def d_relu(self, x):\n","        return np.where( x > 0, 1, 0)\n","\n","    # ステップ関数の導関数\n","    def d_step_function(self, x):\n","        return 0\n","\n","    # 最小二乗法の導関数\n","    def d_least_square(d, y):\n","        return y - d\n","\n","\n","    # ソフトマックスとクロスエントロピーの複合導関数\n","    def d_softmax_with_loss(self, d, y):\n","        batch_size = d.shape[0]\n","        if d.size == y.size: # 教師データがone-hot-vectorの場合\n","            dx = (y - d) / batch_size\n","        else:\n","            dx = y.copy()\n","            dx[np.arange(batch_size), d] -= 1\n","            dx = dx / batch_size\n","        return dx\n","\n","    # シグモイドとクロスエントロピーの複合導関数\n","    def d_sigmoid_with_loss(self, d, y):\n","        return y - d\n","\n","    # 数値微分\n","    def numerical_gradient(self, f, x):\n","        h = 1e-4\n","        grad = np.zeros_like(x)\n","\n","        for idx in range(x.size):\n","            tmp_val = x[idx]\n","            # f(x + h)の計算\n","            x[idx] = tmp_val + h\n","            fxh1 = f(x)\n","\n","            # f(x - h)の計算\n","            x[idx] = tmp_val - h\n","            fxh2 = f(x)\n","\n","            grad[idx] = (fxh1 - fxh2) / (2 * h)\n","            # 値を元に戻す\n","            x[idx] = tmp_val\n","\n","        return grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRMjjOPGoAeM"},"source":["順伝播の実装\n","\n"]},{"cell_type":"code","metadata":{"id":"396ySHqoak3n"},"source":["# 順伝播\n","def forward(network, x, activation =\"sigmoid\" ):\n","    \n","    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n","    b1, b2, b3  = network['b1'], network['b2'], network['b3']\n","    \n","    # インスタンス作成\n","    functions = functions()\n","    \n","    if activation == \"sigmoid\":\n","        # 第１層への入力\n","        A1 =  np.dot(x, W1) + b1\n","        # 第１層の出力（第１層の入力を活性化関数に通す）\n","        Z1 = functions.sigmoid(A1)\n","\n","        # 第２層への入力\n","        A2 =  np.dot(Z1, W2) + b2\n","        # 第２層の出力（第２層の入力を活性化関数に通す）\n","        Z2 = functions.sigmoid(A2) \n","    \n","    \n","    elif activation == \"tanh\":\n","        # 第１層への入力\n","        A1 =  np.dot(x, W1) + b1\n","        # 第１層の出力（第１層の入力を活性化関数に通す）\n","        Z1 = np.tanh(A1)\n","\n","        # 第２層への入力\n","        A2 =  np.dot(Z1, W2) + b2\n","        # 第２層の出力（第２層の入力を活性化関数に通す）\n","        Z2 = np.tanh(A2)  \n","        \n","    elif activation == \"relu\" :\n","        # 第１層への入力\n","        A1 =  np.dot(x, W1) + b1\n","        # 第１層の出力（第１層の入力を活性化関数に通す）\n","        Z1 = functions.relu(A1)\n","\n","        # 第２層への入力\n","        A2 =  np.dot(Z1, W2) + b2\n","        # 第２層の出力（第２層の入力を活性化関数に通す）\n","        Z2 = functions.relu(A2)   \n","    \n","    # 第３層への入力\n","    A3 =  np.dot(Z2, W3) + b3\n","    # 出力層への出力\n","    y_pred = functions.softmax(A3)\n"," \n","    return Z1, Z2, y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fGNZgHlGsYo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lF-8OqVahm3"},"source":["【問題3】交差エントロピー誤差の実装\n","目的関数（損失関数）を作成します。\n","\n","\n","多クラス分類の目的関数である交差エントロピー誤差 $L$ は次の数式です。\n","$$\n","L = - \\frac{1}{n_b}\\sum_{j}^{n_b}\\sum_{k}^{n_c}y_{jk} log(z_{3\\_jk})\n","$$\n","\n","$y_{ij}$ : $j$ 番目のサンプルの $k$ 番目のクラスの正解ラベル（one-hot表現で0か1のスカラー）\n","\n","\n","$z_{3_ij}$ : $j$ 番目のサンプルの $k$ 番目のクラスの確率（スカラー）\n","\n","\n","$n_{b}$ : バッチサイズ、batch_size\n","\n","\n","$n_{c}$ : クラスの数、n_output（今回のMNISTでは10）\n","\n","\n","サンプル1つあたりの誤差が求まります。\n","\n","\n","実数におけるlog(x)の定義域は0 < xです。したがって、logの中身がとても小さい値になってしまったときエラーを起こします。そこでlogの中に1e-7を足すことでエラーを回避できます。\n","\n","\n","こういった処理はlogに限らず、さまざまな場所で出てくることがあります。"]},{"cell_type":"code","metadata":{"id":"kGKRkDJkoX6b"},"source":["# クロスエントロピー\n","def cross_entropy_error(d, y):\n","    if y.ndim == 1:\n","        d = d.reshape(1, d.size)\n","        y = y.reshape(1, y.size)\n","\n","    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","    if d.size == y.size:\n","        d = d.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_GTHyrq_ahtW"},"source":["【問題4】バックプロパゲーションの実装\n","三層のニューラルネットワークのバックプロパゲーションを作成してください。確率的勾配降下法を行う部分です。\n","\n","\n","数式を以下に示します。\n","\n","\n","まず、i層目の重みとバイアスの更新式です。 $W_i$ と $B_i$ に対し、更新後の $W_i^{\\prime}$ と $B_i^{\\prime}$ は次の数式で求められます。\n","\n","$$\n","W_i^{\\prime} = W_i - \\alpha \\frac{\\partial L}{\\partial W_i} \\\\\n","B_i^{\\prime} = B_i - \\alpha \\frac{\\partial L}{\\partial B_i}\n","$$\n","\n","$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n","\n","\n","$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n","\n","\n","$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n","\n","\n","＊この勾配はミニバッチのサンプル数分の合計または平均を考えます。ここでは合計を計算します。\n","\n","\n","この更新方法はSprint3線形回帰やsprint4ロジスティック回帰における最急降下法と同様です。より効果的な更新方法が知られており、それは次のSprintで扱います。\n","\n","\n","勾配 $\\frac{\\partial L}{\\partial W_i}$ や $\\frac{\\partial L}{\\partial B_i}$ を求めるために、バックプロパゲーションを行います。以下の数式です。ハイパボリックタンジェント関数を使用した例を載せました。シグモイド関数の場合の数式はその後ろにあります。\n","\n","\n","「3層目」\n","$$\n","\\frac{\\partial L}{\\partial A_3} = \\frac{1}{n_b}(Z_{3} - Y)\\\\\n","\\frac{\\partial L}{\\partial B_3} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{3\\_j}}\\\\\n","\\frac{\\partial L}{\\partial W_3} = Z_{2}^{T}\\cdot \\frac{\\partial L}{\\partial A_3}\\\\\n","\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial A_3} \\cdot W_3^T\n","$$\n","\n","$\\frac{\\partial L}{\\partial A_3}$ : $A_3$ に関する損失 $L$ の勾配 (batch_size, n_output)\n","\n","\n","$\\frac{\\partial L}{\\partial A_{3_j}}$ : j番目のサンプルの$A_3$ に関する損失 $L$ の勾配 (n_nodes2,)\n","\n","\n","$\\frac{\\partial L}{\\partial B_3}$ : $B_3$ に関する損失 $L$ の勾配 (n_output,)\n","\n","\n","$\\frac{\\partial L}{\\partial W_3}$ : $W_3$ に関する損失 $L$ の勾配 (n_nodes2, n_output)\n","\n","\n","$\\frac{\\partial L}{\\partial Z_2}$ : $Z_2$ に関する損失 $L$ の勾配 (batch_size, n_nodes2)\n","\n","\n","$Z_{3}$ : ソフトマックス関数の出力 (batch_size, n_nodes2)\n","\n","\n","$Y$ : 正解ラベル (batch_size, n_output)\n","\n","\n","$Z_{2}$ : 2層目の活性化関数の出力 (batch_size, n_nodes2)\n","\n","\n","$W_3$ : 3層目の重み (n_nodes2, n_output)\n","\n","\n","$n_{b}$ : バッチサイズ、batch_size\n","\n","\n","「2層目」\n","\n","$$\n","\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_2} \\odot \\{1-tanh^2(A_{2})\\}\\\\\n","\\frac{\\partial L}{\\partial B_2} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{2\\_j}}\\\\\n","\\frac{\\partial L}{\\partial W_2} = Z_{1}^T \\cdot \\frac{\\partial L}{\\partial A_2}\\\\\n","\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_2} \\cdot W_2^T\n","$$\n","\n","$\\frac{\\partial L}{\\partial A_2}$ : $A_2$ に関する損失 $L$ の勾配 (batch_size, n_nodes2)\n","\n","\n","$\\frac{\\partial L}{\\partial A_{2_j}}$ : j番目のサンプルの$A_2$ に関する損失 $L$ の勾配 (n_nodes2,)\n","\n","\n","$\\frac{\\partial L}{\\partial B_2}$ : $B_2$ に関する損失 $L$ の勾配 (n_nodes2,)\n","\n","\n","$\\frac{\\partial L}{\\partial W_2}$ : $W_2$ に関する損失 $L$ の勾配 (n_nodes1, n_nodes2)\n","\n","\n","$\\frac{\\partial L}{\\partial Z_2}$ : $Z_2$ に関する損失 $L$ の勾配 (batch_size, n_nodes2)\n","\n","\n","$A_2$ : 2層目の出力 (batch_size, n_nodes2)\n","\n","\n","$Z_{1}$ : 1層目の活性化関数の出力 (batch_size, n_nodes1)\n","\n","\n","$W_2$ : 2層目の重み (n_nodes1, n_nodes2)\n","\n","\n","「1層目」\n","$$\n","\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_1} \\odot \\{1-tanh^2(A_{1})\\}\\\\\n","\\frac{\\partial L}{\\partial B_1} = \\sum_{j}^{n_b}\\frac{\\partial L}{\\partial A_{1\\_j}}\\\\\n","\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\frac{\\partial L}{\\partial A_1}\\\\\n","$$\n","\n","$\\frac{\\partial L}{\\partial A_1}$ : $A_1$ に関する損失 $L$ の勾配 (batch_size, n_nodes1)\n","\n","\n","$\\frac{\\partial L}{\\partial A_{1_j}}$ : j番目のサンプルの$A_1$ に関する損失 $L$ の勾配 (n_nodes1,)\n","\n","\n","$\\frac{\\partial L}{\\partial B_1}$ : $B_1$ に関する損失 $L$ の勾配 (n_nodes1,)\n","\n","\n","$\\frac{\\partial L}{\\partial W_1}$ : $W_1$ に関する損失 $L$ の勾配 (n_features, n_nodes1)\n","\n","\n","$\\frac{\\partial L}{\\partial Z_1}$ : $Z_1$ に関する損失 $L$ の勾配 (batch_size, n_nodes1)\n","\n","\n","$A_1$ : 1層目の出力 (batch_size, n_nodes1)\n","\n","\n","$X$ : 特徴量ベクトル (batch_size, n_features)\n","\n","\n","$W_1$ : 1層目の重み (n_features, n_nodes1)\n","\n","\n","《補足》\n","\n","\n","活性化関数にシグモイド関数を使用した場合は、次のようになります。\n","\n","$$\n","\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_2} \\odot  \\{1-sigmoid(A_{2})\\}sigmoid(A_{2})\n","\\\\\n","\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_1} \\odot  \\{1-sigmoid(A_{1})\\}sigmoid(A_{1})\n","$$"]},{"cell_type":"code","metadata":{"id":"2geocFRYak5Z"},"source":["# 誤差逆伝播\n","def backward(x, y, z1, z2, y_pred, activation =\"sigmoid\" ):\n","    grad = {}\n","    \n","    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n","    b1, b2, b3 = network['b1'], network['b2'], network['b3']   \n","    \n","    # 出力層でのデルタ\n","    delta3 = functions.d_softmax_with_loss(y, y_pred)\n","    \n","    if activation == \"sigmoid\":\n","        # b3の勾配\n","        grad['b3'] = np.sum(delta3, axis=0)\n","\n","        # W3の勾配\n","        grad['W3'] = np.dot(z2.T, delta3)\n","        \n","        # 2層でのデルタ\n","        delta2 = np.dot(delta3, W3.T) * functions.d_sigmoid(z2)\n","        \n","        # b2の勾配\n","        grad['b2'] = np.sum(delta2, axis=0)\n","\n","        # W2の勾配\n","        grad['W2'] = np.dot(z1.T, delta2)\n","\n","        # 1層でのデルタ\n","        delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)\n","\n","        # b1の勾配\n","        grad['b1'] = np.sum(delta1, axis=0)\n","\n","        # W1の勾配\n","        grad['W1'] = np.dot(x.T, delta1)\n","        \n","    elif activation == \"relu\":\n","        # b2の勾配\n","        grad['b2'] = np.sum(delta2, axis=0)\n","\n","        # W2の勾配\n","        grad['W2'] = np.dot(z1.T, delta2)\n","\n","        # 1層でのデルタ\n","        delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\n","\n","        # b1の勾配\n","        grad['b1'] = np.sum(delta1, axis=0)\n","\n","        # W1の勾配\n","        grad['W1'] = np.dot(x.T, delta1)\n","    \n","\n","    for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","        network[key]  -= learning_rate * grad[key]\n","    \n","    return grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iLJ3sVZomK_"},"source":["def fit(self, X, y, X_val=None, y_val=None):\n","    \"\"\"\n","    ニューラルネットワーク分類器を学習する。\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","        訓練データの特徴量\n","    y : 次の形のndarray, shape (n_samples, )\n","        訓練データの正解値\n","    X_val : 次の形のndarray, shape (n_samples, n_features)\n","        検証データの特徴量\n","    y_val : 次の形のndarray, shape (n_samples, )\n","        検証データの正解値\n","    \"\"\"\n","\n","    epoc = 10\n","    # ▼▼▼　メイン処理\n","    for i in range(epoc):\n","        # ミニバッチ取得\n","        get_mini_batch = GetMiniBatch(X, y_train, batch_size=20)\n","        print(len(get_mini_batch)) # 2400\n","        print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n","        for mini_X_train, mini_y_train in get_mini_batch:\n","            # このfor文内でミニバッチが使える\n","\n","            z1, z2, y_pred = forward(network, mini_X_train)\n","            grad = backward(mini_X_train, mini_y_train, z1, z2, y_pred)\n","\n","    if self.verbose:\n","        #verboseをTrueにした際は学習過程などを出力する\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDGbeNJ0ahz0"},"source":["【問題5】推定\n","推定を行うメソッドを作成してください。\n","\n","\n","フォワードプロパゲーションによって出力された10個の確率の中で、最も高いものはどれかを判定します。\n","\n","\n","numpy.argmax — NumPy v1.17 Manual"]},{"cell_type":"code","metadata":{"id":"SR89qXvlalgK"},"source":["def predict(self, X):\n","        \"\"\"\n","        ニューラルネットワーク分類器を使い推定する。\n","\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","\n","        Returns\n","        -------\n","            次の形のndarray, shape (n_samples, 1)\n","            推定結果\n","        \"\"\"\n","        z1, z2, y_pred = forward(network, X)\n","        \n","        return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vg2V5kSjo190"},"source":["以上をクラスタ化する"]},{"cell_type":"code","metadata":{"id":"eNiPwRWvd9WX"},"source":["from sklearn.preprocessing import OneHotEncoder\n","class ScratchSimpleNeuralNetrowkClassifier():\n","    \"\"\"\n","    シンプルな三層ニューラルネットワーク分類器\n","\n","    Parameters\n","    ----------\n","\n","    Attributes\n","    ----------\n","    \"\"\"\n","    def __init__(self, verbose = True, activation = \"sigmoid\", initialize = \"None\", optimaizer = \"SGD\", epoc = 50, lr = 0.001, sigma = 1):\n","        self.verbose = verbose\n","        \n","        self.batch_size = 20 # バッチサイズ\n","        self.n_features = 784 # 特徴量の数\n","        self.n_nodes1 = 400 # 1層目のノード数\n","        self.n_nodes2 = 200 # 2層目のノード数\n","        self.n_output = 10 # 出力のクラス数（3層目のノード数）\n","        \n","        self.sigma = sigma # ガウス分布の標準偏差\n","        \n","        self.network = {} \n","        if initialize == \"None\":\n","            self.network['W1'] = self.sigma * np.random.randn(self.n_features, self.n_nodes1)\n","            self.network['W2'] = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)        \n","            self.network['W3'] = self.sigma * np.random.randn(self.n_nodes2, self.n_output)  \n","            \n","        elif  initialize == \"Xavier\":\n","            # Xavierの初期値\n","            self.network['W1'] = np.random.rand(self.n_features, self.n_nodes1) / np.sqrt(self.n_features)\n","            self.network['W2'] = np.random.rand(self.n_nodes1, self.n_nodes2) / np.sqrt(self.n_nodes1)\n","            self.network['W3'] = np.random.rand(self.n_nodes2, self.n_output) / np.sqrt(self.n_nodes2)\n","            \n","        elif  initialize == \"He\":\n","            # Xavierの初期値\n","            self.network['W1'] = np.random.rand(self.n_features, self.n_nodes1) / np.sqrt(self.n_features) * np.sqrt(2)\n","            self.network['W2'] = np.random.rand(self.n_nodes1, self.n_nodes2) / np.sqrt(self.n_nodes1) * np.sqrt(2)\n","            self.network['W3'] = np.random.rand(self.n_nodes2, self.n_output) / np.sqrt(self.n_nodes2) * np.sqrt(2)\n","            \n","\n","        \n","        self.network['b1'] = np.zeros(self.n_nodes1)\n","        self.network['b2'] = np.zeros(self.n_nodes2)\n","        self.network['b3'] = np.zeros(self.n_output)\n","        # self.learning_rate = 0.0002\n","        self.learning_rate = lr\n","        self.epoc = epoc\n","        self.activation = activation\n","        self.func = functions() \n","        self.train_loss_list = []\n","        self.test_loss_list = []\n","        self.plot_interval = 100\n","        self.optimaizer = optimaizer\n","        self.h = {}\n","        self.m = {}\n","        self.v = {}        \n","        \n","        print(\"self.learning_rate = \", self.learning_rate)\n","        print(\"self.activation = \", self.activation)\n","        print(\"self.optimaizer = \", self.optimaizer)\n","\n","    def fit(self, X, y, X_val=None, y_val=None):\n","        \"\"\"\n","        ニューラルネットワーク分類器を学習する。\n","\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            訓練データの特徴量\n","        y : 次の形のndarray, shape (n_samples, )\n","            訓練データの正解値\n","        X_val : 次の形のndarray, shape (n_samples, n_features)\n","            検証データの特徴量\n","        y_val : 次の形のndarray, shape (n_samples, )\n","            検証データの正解値\n","        \"\"\"\n","\n","        print(\"Learning Start!\")\n","        \n","        # one-hot-vectol化\n","        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n","        \n","        if y_val is not None:\n","            y_test_one_hot = enc.transform(y_val[:, np.newaxis])\n","        \n","        # 学習回数のカウンタ\n","        learning_cnt = 0\n","        \n","        for i in range(self.epoc):\n","            # ミニバッチ取得\n","            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size = self.batch_size)\n","            \n","            # すべてのミニバッチを抜ける前に直前の値をバックアップしておく(グラフ用に)\n","            bk_mini_y_train = 0\n","            bk_y_pred = 0\n","            train_loss_batch = []\n","            test_loss_batch = []\n","            for mini_X_train, mini_y_train in get_mini_batch:\n","                # このfor文内でミニバッチが使える\n","                \n","                z1, z2, y_pred = self._forward(self.network, mini_X_train, activation = self.activation)                \n","                grad = self._backward(learning_cnt, mini_X_train, mini_y_train, z1, z2, y_pred, activation = self.activation)\n","                \n","                learning_cnt += 1\n","                \n","                # ミニバッチ内のロスを格納\n","                train_loss_batch.append(self.func.cross_entropy_error(mini_y_train, y_pred))\n","     \n","\n","            # loss計算\n","            # ミニバッチ内のロスの平均を取る\n","            train_loss_mean = np.array(train_loss_batch).mean()\n","            self.train_loss_list.append(train_loss_mean)\n","\n","            # test_loss の初期化\n","            test_loss = 0\n","            if X_val is not None:\n","                z1, z2, y_test_pred = self._forward(self.network, X_val)\n","                test_loss = self.func.cross_entropy_error(y_test_one_hot, y_test_pred)\n","                #self.test_loss_list.append(self.func.cross_entropy_error(y_test_one_hot, y_test_pred))\n","                self.test_loss_list.append(test_loss)\n","\n","            if self.verbose:\n","                #verboseをTrueにした際は学習過程などを出力する\n","                print(\"Epoc Count = {}, train_loss = {}, test_loss = {}\".format(i+1, train_loss_mean, test_loss))\n","                \n","        print(\"Learning Finish!\")\n","\n","    def predict(self, X):\n","        \"\"\"\n","        ニューラルネットワーク分類器を使い推定する。\n","\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","\n","        Returns\n","        -------\n","            次の形のndarray, shape (n_samples, 1)\n","            推定結果\n","        \"\"\"\n","        z1, z2, y_pred = self._forward(self.network, X)\n","\n","        y_pred = np.argmax(y_pred, axis = 1)\n","        return y_pred\n","    \n","    \n","\n","    # 順伝播\n","    def _forward(self, network, x, activation =\"sigmoid\" ):\n","\n","        W1, W2, W3 = self.network['W1'], self.network['W2'], self.network['W3']\n","        b1, b2, b3 = self.network['b1'], self.network['b2'], self.network['b3']\n","\n","        if activation == \"sigmoid\":\n","            # 第１層への入力\n","            A1 =  np.dot(x, W1) + b1\n","            # 第１層の出力（第１層の入力を活性化関数に通す）\n","            Z1 = self.func.sigmoid(A1)\n","\n","            # 第２層への入力\n","            A2 =  np.dot(Z1, W2) + b2\n","            # 第２層の出力（第２層の入力を活性化関数に通す）\n","            Z2 = self.func.sigmoid(A2) \n","\n","        elif activation == \"tanh\":\n","            # 第１層への入力\n","            A1 =  np.dot(x, W1) + b1\n","            # 第１層の出力（第１層の入力を活性化関数に通す）\n","            Z1 = np.tanh(A1)\n","\n","            # 第２層への入力\n","            A2 =  np.dot(Z1, W2) + b2\n","            # 第２層の出力（第２層の入力を活性化関数に通す）\n","            Z2 = np.tanh(A2)  \n","\n","        elif activation == \"relu\" :\n","            # 第１層への入力\n","            A1 =  np.dot(x, W1) + b1\n","            # 第１層の出力（第１層の入力を活性化関数に通す）\n","            Z1 = self.func.relu(A1)\n","\n","            # 第２層への入力\n","            A2 =  np.dot(Z1, W2) + b2\n","            # 第２層の出力（第２層の入力を活性化関数に通す）\n","            Z2 = self.func.relu(A2)   \n","\n","        # 第３層への入力\n","        A3 =  np.dot(Z2, W3) + b3\n","        # 出力層への出力\n","        y_pred = self.func.softmax(A3)\n","\n","        return Z1, Z2, y_pred\n","\n","    # 誤差逆伝播\n","    def _backward(self, i, x, y, z1, z2, y_pred, activation =\"sigmoid\" ):\n","        grad = {}\n","\n","        W1, W2, W3 = self.network['W1'], self.network['W2'], self.network['W3']\n","        b1, b2, b3 = self.network['b1'], self.network['b2'], self.network['b3']   \n","\n","        # 出力層でのデルタ\n","        delta3 = self.func.d_softmax_with_loss(y, y_pred)\n","\n","        if activation == \"sigmoid\":\n","            # b3の勾配\n","            grad['b3'] = np.sum(delta3, axis=0)\n","\n","            # W3の勾配\n","            grad['W3'] = np.dot(z2.T, delta3)\n","\n","            # 2層でのデルタ\n","            delta2 = np.dot(delta3, W3.T) * self.func.d_sigmoid(z2)\n","\n","            # b2の勾配\n","            grad['b2'] = np.sum(delta2, axis=0)\n","\n","            # W2の勾配\n","            grad['W2'] = np.dot(z1.T, delta2)\n","\n","            # 1層でのデルタ\n","            delta1 = np.dot(delta2, W2.T) * self.func.d_sigmoid(z1)\n","\n","            # b1の勾配\n","            grad['b1'] = np.sum(delta1, axis=0)\n","\n","            # W1の勾配\n","            grad['W1'] = np.dot(x.T, delta1)\n","\n","            \n","        elif activation == \"relu\":\n","            # b3の勾配\n","            grad['b3'] = np.sum(delta3, axis=0)\n","\n","            # W3の勾配\n","            grad['W3'] = np.dot(z2.T, delta3)\n","            \n","            # 2層でのデルタ\n","            delta2 = np.dot(delta3, W3.T) * self.func.d_relu(z2)\n","            \n","            # b2の勾配\n","            grad['b2'] = np.sum(delta2, axis=0)\n","\n","            # W2の勾配\n","            grad['W2'] = np.dot(z1.T, delta2)\n","\n","            # 1層でのデルタ\n","            delta1 = np.dot(delta2, W2.T) * self.func.d_relu(z1)\n","\n","            # b1の勾配\n","            grad['b1'] = np.sum(delta1, axis=0)\n","\n","            # W1の勾配\n","            grad['W1'] = np.dot(x.T, delta1)\n","\n","     \n","        # =========================================================\n","         # パラメータに勾配適用\n","        if self.optimaizer == \"AdaGrad\":\n","        # AdaGrad =====================================================\n","            # if i == 0:\n","                # self.h = {}\n","            for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","                if i == 0:\n","                    self.h[key] = np.zeros_like(self.network[key])\n","                self.h[key] += np.square(grad[key])\n","                self.network[key] -= self.learning_rate * grad[key] / (np.sqrt(self.h[key]) + 1e-7)  \n","                \n","        # =========================================================\n","         # パラメータに勾配適用\n","        elif self.optimaizer == \"Adam\":\n","        # Adam ====================================================\n","            # print(\"i = \", i)\n","            beta1 = 0.9\n","            beta2 = 0.999 \n","            # if i == 0:\n","                # self.m = {}\n","                # self.v = {}\n","            learning_rate_t  = self.learning_rate * np.sqrt(1.0 - beta2 ** (i + 1)) / (1.0 - beta1 ** (i + 1))    \n","            for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","                if i == 0:\n","                    self.m[key] = np.zeros_like(self.network[key])\n","                    self.v[key] = np.zeros_like(self.network[key])\n","\n","                self.m[key] += (1 - beta1) * (grad[key] - self.m[key])\n","                self.v[key] += (1 - beta2) * (grad[key] ** 2 - self.v[key])            \n","                self.network[key] -= learning_rate_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)   \n","                \n","                \n","        else:\n","            # その他はすべてSGDにする\n","            # SGD =====================================================\n","            for key in ('W1', 'W2', 'W3', 'b1', 'b2', 'b3'):\n","                self.network[key]  -= self.learning_rate * grad[key]                \n","                \n","                \n","        return grad\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fd5O_zTah6I"},"source":["4.検証\n","\n","【問題6】学習と推定\n","MNISTのデータを学習・推定し、Accuracyを計算してください。\n","\n"]},{"cell_type":"code","metadata":{"id":"6hlMdqDsal78","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629296583274,"user_tz":-540,"elapsed":225,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"e4a91708-508e-4319-b38e-77199d7e588f"},"source":["ScratchMNIST = ScratchSimpleNeuralNetrowkClassifier(verbose = True, activation = \"sigmoid\", epoc = 50, lr = 0.08, sigma = 0.05)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["self.learning_rate =  0.08\n","self.activation =  sigmoid\n","self.optimaizer =  SGD\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugcBf8uTpZvX","executionInfo":{"status":"ok","timestamp":1629297530878,"user_tz":-540,"elapsed":896640,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"50048244-8183-4be5-f266-f84cbd6b249f"},"source":["ScratchMNIST.fit(X_train, y_train, X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Learning Start!\n","Epoc Count = 1, train_loss = 0.9630347468515401, test_loss = 0.3959620794360402\n","Epoc Count = 2, train_loss = 0.360315588067102, test_loss = 0.3110944988009129\n","Epoc Count = 3, train_loss = 0.3062483971814988, test_loss = 0.2794866796444407\n","Epoc Count = 4, train_loss = 0.2810605213886924, test_loss = 0.2637220112301405\n","Epoc Count = 5, train_loss = 0.26646607806906836, test_loss = 0.2548478470469067\n","Epoc Count = 6, train_loss = 0.25649221011671264, test_loss = 0.2493159995769047\n","Epoc Count = 7, train_loss = 0.24898381698860395, test_loss = 0.24560450399913275\n","Epoc Count = 8, train_loss = 0.24300176996250344, test_loss = 0.24297265701086534\n","Epoc Count = 9, train_loss = 0.23802871516144375, test_loss = 0.24099797330183473\n","Epoc Count = 10, train_loss = 0.23374049601440097, test_loss = 0.23941497255188243\n","Epoc Count = 11, train_loss = 0.2299210824824915, test_loss = 0.23805232126106726\n","Epoc Count = 12, train_loss = 0.22642279581292124, test_loss = 0.23680036731585297\n","Epoc Count = 13, train_loss = 0.22314473083361472, test_loss = 0.23558938196410145\n","Epoc Count = 14, train_loss = 0.22002003918883872, test_loss = 0.23437211331466445\n","Epoc Count = 15, train_loss = 0.21700715875217738, test_loss = 0.23310971371075292\n","Epoc Count = 16, train_loss = 0.214082306759112, test_loss = 0.23177145882341274\n","Epoc Count = 17, train_loss = 0.21123270129054889, test_loss = 0.2303488899506503\n","Epoc Count = 18, train_loss = 0.208449810139438, test_loss = 0.22885598577516558\n","Epoc Count = 19, train_loss = 0.20572255369636214, test_loss = 0.22731320417289963\n","Epoc Count = 20, train_loss = 0.20303403731353908, test_loss = 0.22573943432681626\n","Epoc Count = 21, train_loss = 0.20036337935764004, test_loss = 0.22415693765967015\n","Epoc Count = 22, train_loss = 0.19768802426379878, test_loss = 0.22258392329668736\n","Epoc Count = 23, train_loss = 0.1949860869817652, test_loss = 0.22101247759145645\n","Epoc Count = 24, train_loss = 0.1922483988449236, test_loss = 0.21941346290583005\n","Epoc Count = 25, train_loss = 0.18949249282655628, test_loss = 0.21775722203196607\n","Epoc Count = 26, train_loss = 0.18675099335418463, test_loss = 0.2160111933785295\n","Epoc Count = 27, train_loss = 0.1840481766779576, test_loss = 0.21417664363592767\n","Epoc Count = 28, train_loss = 0.18140160242589679, test_loss = 0.21232488531775304\n","Epoc Count = 29, train_loss = 0.17882438850611512, test_loss = 0.21051261004856264\n","Epoc Count = 30, train_loss = 0.17631027565117313, test_loss = 0.2087146609603687\n","Epoc Count = 31, train_loss = 0.17384632284004756, test_loss = 0.20688861154715932\n","Epoc Count = 32, train_loss = 0.17142487312318014, test_loss = 0.20503689800901667\n","Epoc Count = 33, train_loss = 0.16904695304431547, test_loss = 0.2032057863228701\n","Epoc Count = 34, train_loss = 0.16674183411287197, test_loss = 0.20146752824878972\n","Epoc Count = 35, train_loss = 0.16456030391070234, test_loss = 0.19988875678352078\n","Epoc Count = 36, train_loss = 0.1625363367694083, test_loss = 0.19854064991508677\n","Epoc Count = 37, train_loss = 0.1606695024201005, test_loss = 0.19752526108221902\n","Epoc Count = 38, train_loss = 0.15893666064552547, test_loss = 0.1969387275170753\n","Epoc Count = 39, train_loss = 0.15730327464724664, test_loss = 0.19677483018869973\n","Epoc Count = 40, train_loss = 0.15573301807129045, test_loss = 0.19690419423379263\n","Epoc Count = 41, train_loss = 0.15420495361389783, test_loss = 0.1971816749431461\n","Epoc Count = 42, train_loss = 0.15273293905932733, test_loss = 0.19750237409446325\n","Epoc Count = 43, train_loss = 0.15136042848280615, test_loss = 0.1978140413971956\n","Epoc Count = 44, train_loss = 0.15012724990233242, test_loss = 0.1981043305008159\n","Epoc Count = 45, train_loss = 0.14904221597753847, test_loss = 0.19836065972859385\n","Epoc Count = 46, train_loss = 0.1480839265694285, test_loss = 0.19855131780522642\n","Epoc Count = 47, train_loss = 0.14721925247609496, test_loss = 0.19863179020905344\n","Epoc Count = 48, train_loss = 0.1464179007499068, test_loss = 0.19856241341911982\n","Epoc Count = 49, train_loss = 0.14565647646637003, test_loss = 0.19832420454919136\n","Epoc Count = 50, train_loss = 0.14492015522644738, test_loss = 0.19792745662694114\n","Learning Finish!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxETS6rDurdw","executionInfo":{"status":"ok","timestamp":1629298019624,"user_tz":-540,"elapsed":595,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"507934dc-4c5c-4659-9fcc-ba7cb58654aa"},"source":["y_pred = ScratchMNIST.predict(X_test)\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([7, 2, 1, ..., 4, 5, 6])"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"2YMMpH8FursO","executionInfo":{"status":"ok","timestamp":1629298041172,"user_tz":-540,"elapsed":253,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"2cabacff-5c47-4ee8-ec3a-1c69a684e90c"},"source":["import pandas as pd\n","from sklearn.metrics import classification_report\n","cr = classification_report(y_test, y_pred, output_dict=True)\n","\n","# pandas.DataFrameへ変換\n","df_cr = pd.DataFrame(cr)\n","df_cr"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>accuracy</th>\n","      <th>macro avg</th>\n","      <th>weighted avg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>precision</th>\n","      <td>0.952522</td>\n","      <td>0.975546</td>\n","      <td>0.942363</td>\n","      <td>0.905983</td>\n","      <td>0.917795</td>\n","      <td>0.971250</td>\n","      <td>0.949223</td>\n","      <td>0.958539</td>\n","      <td>0.932712</td>\n","      <td>0.946502</td>\n","      <td>0.9449</td>\n","      <td>0.945244</td>\n","      <td>0.945412</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>0.982653</td>\n","      <td>0.984141</td>\n","      <td>0.950581</td>\n","      <td>0.944554</td>\n","      <td>0.966395</td>\n","      <td>0.871076</td>\n","      <td>0.956159</td>\n","      <td>0.944553</td>\n","      <td>0.925051</td>\n","      <td>0.911794</td>\n","      <td>0.9449</td>\n","      <td>0.943696</td>\n","      <td>0.944900</td>\n","    </tr>\n","    <tr>\n","      <th>f1-score</th>\n","      <td>0.967353</td>\n","      <td>0.979825</td>\n","      <td>0.946454</td>\n","      <td>0.924867</td>\n","      <td>0.941468</td>\n","      <td>0.918440</td>\n","      <td>0.952678</td>\n","      <td>0.951494</td>\n","      <td>0.928866</td>\n","      <td>0.928824</td>\n","      <td>0.9449</td>\n","      <td>0.944027</td>\n","      <td>0.944743</td>\n","    </tr>\n","    <tr>\n","      <th>support</th>\n","      <td>980.000000</td>\n","      <td>1135.000000</td>\n","      <td>1032.000000</td>\n","      <td>1010.000000</td>\n","      <td>982.000000</td>\n","      <td>892.000000</td>\n","      <td>958.000000</td>\n","      <td>1028.000000</td>\n","      <td>974.000000</td>\n","      <td>1009.000000</td>\n","      <td>0.9449</td>\n","      <td>10000.000000</td>\n","      <td>10000.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    0            1  ...     macro avg  weighted avg\n","precision    0.952522     0.975546  ...      0.945244      0.945412\n","recall       0.982653     0.984141  ...      0.943696      0.944900\n","f1-score     0.967353     0.979825  ...      0.944027      0.944743\n","support    980.000000  1135.000000  ...  10000.000000  10000.000000\n","\n","[4 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"markdown","metadata":{"id":"rlvE9k5Du4u-"},"source":["accuracyは、94%となった。"]},{"cell_type":"markdown","metadata":{"id":"ieGUbVOvaiA4"},"source":["【問題7】学習曲線のプロット\n","学習曲線をプロットしてください。\n","\n","\n","ニューラルネットワークは過学習が発生しやすいため、学習曲線の確認が重要です。訓練データと検証データに対するエポックごとの損失（交差エントロピー誤差）を記録できるようにする必要があります。\n","\n"]},{"cell_type":"code","metadata":{"id":"FbPDoVIWambB","colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"status":"ok","timestamp":1629298092650,"user_tz":-540,"elapsed":254,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"f49905c6-f9f0-4bf4-edda-526008f02493"},"source":["plt.plot(range(len(ScratchMNIST.train_loss_list)), ScratchMNIST.train_loss_list, color = \"blue\")\n","plt.plot(range(len(ScratchMNIST.test_loss_list)), ScratchMNIST.test_loss_list, color = \"red\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f8761d93a50>]"]},"metadata":{"tags":[]},"execution_count":122},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}