{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Simpleconv1d.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFnr6rNPWvagaWWS+MENiO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VaIXMhPv9ZE4"},"source":["Sprintの目的\n","スクラッチを通してCNNの基礎を理解する\n","\n","どのように学ぶか\n","スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。\n","\n","\n","\n","2.1次元の畳み込みニューラルネットワークスクラッチ\n","\n","畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n","\n","\n","このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n","\n","\n","クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n","\n","\n","1次元畳み込み層とは\n","CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n","\n","\n","畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n","\n","\n","データセットの用意\n","検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"]},{"cell_type":"markdown","metadata":{"id":"0huJIp-o9ZId"},"source":["【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n","チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n","\n","\n","ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n","\n","\n","フォワードプロパゲーションの数式は以下のようになります。\n","\n","\n","$$\n","a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n","$$\n","\n","$a_i$ : 出力される配列のi番目の値\n","\n","\n","$F$ : フィルタのサイズ\n","\n","\n","$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n","\n","\n","$w_s$ : 重みの配列のs番目の値\n","\n","\n","$b$ : バイアス項\n","\n","\n","すべてスカラーです。\n","\n","\n","次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n","\n","$$\n","w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n","b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n","$$\n","\n","$\\alpha$ : 学習率\n","\n","\n","$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n","\n","\n","$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n","\n","\n","勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n","\n","$$\n","\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n","\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n","$$\n","\n","$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n","\n","\n","$N_{out}$ : 出力のサイズ\n","\n","\n","前の層に流す誤差の数式は以下です。\n","\n","$$\n","\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n","$$\n","\n","$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n","\n","\n","ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n","\n","\n","全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差をすべて足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"]},{"cell_type":"code","metadata":{"id":"pU2PeMta97Jm","executionInfo":{"status":"ok","timestamp":1630564550358,"user_tz":-540,"elapsed":2849,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["## ライブラリのimport\n","import numpy as np\n","import math\n","from keras.datasets import mnist\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5n9ZzDynBm6a"},"source":["# 学習に使用するその他クラスの定義\n","\n","これまでのsprintで定義してきたものを流用します。"]},{"cell_type":"code","metadata":{"id":"q0wR-GTwBn1e","executionInfo":{"status":"ok","timestamp":1630564551222,"user_tz":-540,"elapsed":867,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#活性化関数\n","class Sigmoid:\n","    \n","    def forward(self, A):\n","        self.A = A\n","        return self.sigmoid(A)\n","    \n","    def backward(self, dZ):\n","        _sig = self.sigmoid(self.A)\n","        return dZ * (1 - _sig)*_sig\n","    \n","    def sigmoid(self, X):\n","        return 1 / (1 + np.exp(-X))\n","\n","#活性化関数\n","class Tanh:\n","    \n","    def forward(self, A):\n","        self.A = A\n","        return np.tanh(A)\n","    \n","    def backward(self, dZ):\n","        return dZ * (1 - (np.tanh(self.A))**2)\n","\n","#活性化関数\n","class Softmax:\n","    \n","    def forward(self, X):\n","        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n","        return self.Z\n","    \n","    def backward(self, Y):\n","        self.loss = self.loss_func(Y)\n","        return self.Z - Y\n","    \n","    def loss_func(self, Y, Z=None):\n","        if Z is None:\n","            Z = self.Z\n","        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n","\n","#活性化関数\n","class ReLU:\n","    \n","    def forward(self, A):\n","        self.A = A\n","        return np.clip(A, 0, None)\n","    \n","    def backward(self, dZ):\n","        return dZ * np.clip(np.sign(self.A), 0, None)\n","\n","#全結合層\n","class FC:\n","\n","    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n","        self.optimizer = optimizer\n","        self.W = initializer.W(n_nodes1, n_nodes2)\n","        self.B = initializer.B(n_nodes2)\n","        \n","    def forward(self, X):\n","        self.X = X\n","        A = X@self.W + self.B\n","        return A\n","    \n","    def backward(self, dA):\n","        dZ = dA@self.W.T\n","        self.dB = np.sum(dA, axis=0)\n","        self.dW = self.X.T@dA\n","        self.optimizer.update(self)\n","        return dZ\n","\n","#Xavier(ザビエル)の初期値\n","# ニューラルネットワークにおける重みの初期値は、学習の精度、速度に大きく影響する非常に重要な問題であり、勾配消失、アクティベーションの偏りなどが起こらないようにしなければいけない。\n","# そこで提案されたのが「Xaiverの初期値」である。\n","# Xavierの初期値によるアクティベーションは、広がった分布となり、高い表現力を有する。\n","# 活性化関数としてシグモイド関数などを用いるときはおすすめである。\n","# 「Xavierの初期値」は層のノードの数によって、作用させる係数を\n","# 変化させます。\n","\n","# たとえば、前層から渡されるノード数がn個であるときには、標準偏差√nで割ってやります。\n","# つまり以下の値を作用させます。\n","\n","# 1n‾√\n","# 1n\n","# つまり、初期値のバラツキについては、各層ごとにノードの数で均一化しているイメージになるかと思います。\n","class XavierInitializer:\n","    \n","    def W(self, n_nodes1, n_nodes2):\n","        self.sigma = math.sqrt(1 / n_nodes1)\n","        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n","        return W\n","    \n","    def B(self, n_nodes2):\n","        B = self.sigma * np.random.randn(n_nodes2)\n","        return B\n","\n","\n","# Heの初期値\n","# Xavierの初期値と共によく使われる初期値として「Heの初期値」があります。\n","# 作用させる値はXavierと似ていますが、標準偏差√(n/2)で割ってやります。\n","# つまり以下の値を作用させます。\n","\n","# 2n‾‾√\n","# 2n\n","# これまで、活性化関数はSigmoid関数を用いていました、Heの初期値は\n","# 活性化関数は、ReLU関数と一緒に使用します。\n","\n","# ReLU関数は今まで触れておりませんでしたが、以下のようなシンプルな関数です。\n","\n","# f(x)=max(0,x)\n","# f(x)=max(0,x)\n","# つまり、xが0以下の値のときは0, xが正の値の場合はそのままxの値をとる関数です。\n","\n","class HeInitializer():\n","    \n","    def W(self, n_nodes1, n_nodes2):\n","        self.sigma = math.sqrt(2 / n_nodes1)\n","        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n","        return W\n","    \n","    def B(self, n_nodes2):\n","        B = self.sigma * np.random.randn(n_nodes2)\n","        return B\n","\n","#最適化手法\n","class SGD:\n","\n","    def __init__(self, lr):\n","        self.lr = lr\n","    \n","    def update(self, layer):\n","        layer.W -= self.lr * layer.dW\n","        layer.B -= self.lr * layer.dB\n","        return\n","\n","#最適化手法\n","class AdaGrad:\n","    \n","    def __init__(self, lr):\n","        self.lr = lr\n","        self.HW = 1\n","        self.HB = 1\n","    \n","    def update(self, layer):\n","        self.HW += layer.dW**2\n","        self.HB += layer.dB**2\n","        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n","        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n","\n","\n","#ミニバッチ（学習させるときにデータを分けて行う手法）\n","# numpyのshuffleとpermutationの違い\n","# numpy Python\n","# python - shuffle vs permute numpy - Stack Overflow\n","\n","# numpyにはshuffle(x)とpermutation(x)というほぼ同じ機能の関数があります．\n","# どちらも，配列をランダムに並び替えますが，違いが2つあります．\n","\n","# ひとつは，shuffle(x)は配列をin-placeで並び替えるが，permutation(x)は並び替えた配列のコピーを生成するという点です．つまり：\n","class GetMiniBatch:\n","\n","    def __init__(self, X, y, batch_size = 20, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self._X = X[shuffle_index]\n","        self._y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","        \n","    def __len__(self):\n","        return self._stop\n","    \n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self._X[p0:p1], self._y[p0:p1] \n","    \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    \n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self._X[p0:p1], self._y[p0:p1]\n","\n","#重みとバイアスの初期化\n","# 可変長引数\n","# Python の関数では引数の個数をいくつでも\n","# 受け取ることができる関数を定義することができます。\n","# 可変長の引数を定義するためには可変長引数の前に\n","# アスタリスク * を付けます。\n","class SimpleInitializer:\n","\n","    def __init__(self, sigma):\n","        self.sigma = sigma\n","        \n","    def W(self, *shape):\n","        W = self.sigma * np.random.randn(*shape) \n","        #引数*shapeは、可変長引数である。よって、Wは、W１、W2、W3・・・とか名前つけなくても、自動でつけられる\n","        return W\n","    \n","    def B(self, *shape):\n","        B = self.sigma * np.random.randn(*shape)\n","        return B\n","\n","# #重みとバイアスの初期化\n","# class SimpleInitializer:\n","\n","#     def __init__(self, sigma):\n","#         self.sigma = sigma\n","        \n","#     def W(self, shape):\n","#         W = self.sigma * np.random.randn(shape)\n","#         return W\n","    \n","#     def B(self, shape):\n","#         B = self.sigma * np.random.randn(shape)\n","#         return B\n","\n","\n","\n","\n","\n","# class functions():\n","#     # 中間層の活性化関数\n","#     # シグモイド関数（ロジスティック関数）\n","#     def sigmoid(self, x):\n","#         return 1/(1 + np.exp(-x))\n","\n","#     # ReLU関数\n","#     def relu(self, x):\n","#         return np.maximum(0, x)\n","\n","#     # ステップ関数（閾値0）\n","#     def step_function(self, x):\n","#         return np.where( x > 0, 1, 0) \n","\n","#     # 出力層の活性化関数\n","#     # ソフトマックス関数\n","#     def softmax(self, x):\n","#         if x.ndim == 2:\n","#             x = x.T\n","#             x = x - np.max(x, axis=0)\n","#             y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","#             return y.T\n","\n","#         x = x - np.max(x) # オーバーフロー対策\n","#         return np.exp(x) / np.sum(np.exp(x))\n","\n","#     # ソフトマックスとクロスエントロピーの複合関数\n","#     def softmax_with_loss(self, d, x):\n","#         y = softmax(x)\n","#         return cross_entropy_error(d, y)\n","\n","#     # 誤差関数\n","#     # 最小二乗法\n","#     def least_square(self, d, y):\n","#         return np.sum(np.square(d - y)) / 2\n","\n","#     # クロスエントロピー\n","#     def cross_entropy_error(self, d, y):\n","#         if y.ndim == 1:\n","#             d = d.reshape(1, d.size)\n","#             y = y.reshape(1, y.size)\n","\n","#         # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n","#         if d.size == y.size:\n","#             d = d.argmax(axis=1)\n","\n","#         batch_size = y.shape[0]\n","#         return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n","\n","\n","\n","#     # 活性化関数の導関数\n","#     # シグモイド関数（ロジスティック関数）の導関数\n","#     def d_sigmoid(self, x):\n","#         dx = (1.0 - self.sigmoid(x)) * self.sigmoid(x)\n","#         return dx\n","\n","#     # ReLU関数の導関数\n","#     def d_relu(self, x):\n","#         return np.where( x > 0, 1, 0)\n","\n","#     # ステップ関数の導関数\n","#     def d_step_function(self, x):\n","#         return 0\n","\n","#     # 最小二乗法の導関数\n","#     def d_least_square(d, y):\n","#         return y - d\n","\n","\n","#     # ソフトマックスとクロスエントロピーの複合導関数\n","#     def d_softmax_with_loss(self, d, y):\n","#         batch_size = d.shape[0]\n","#         if d.size == y.size: # 教師データがone-hot-vectorの場合\n","#             dx = (y - d) / batch_size\n","#         else:\n","#             dx = y.copy()\n","#             dx[np.arange(batch_size), d] -= 1\n","#             dx = dx / batch_size\n","#         return dx\n","\n","#     # シグモイドとクロスエントロピーの複合導関数\n","#     def d_sigmoid_with_loss(self, d, y):\n","#         return y - d\n","\n","#     # 数値微分\n","#     def numerical_gradient(self, f, x):\n","#         h = 1e-4\n","#         grad = np.zeros_like(x)\n","\n","#         for idx in range(x.size):\n","#             tmp_val = x[idx]\n","#             # f(x + h)の計算\n","#             x[idx] = tmp_val + h\n","#             fxh1 = f(x)\n","\n","#             # f(x - h)の計算\n","#             x[idx] = tmp_val - h\n","#             fxh2 = f(x)\n","\n","#             grad[idx] = (fxh1 - fxh2) / (2 * h)\n","#             # 値を元に戻す\n","#             x[idx] = tmp_val\n","\n","#         return grad"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Dfh7zV6BihN","executionInfo":{"status":"ok","timestamp":1630564551222,"user_tz":-540,"elapsed":25,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["class SimpleConv1d():\n","    \"\"\"畳み込みクラス\n","    \"\"\"\n","    def forward(self, x, w, b):\n","        \"\"\"順伝播\n","        Parameters\n","        -----------\n","        x : 入力配列\n","        w : 重み\n","        b : バイアス\n","        \"\"\"\n","        \n","        # 返り値入力配列\n","        a = []\n","        # 1づつずらしながら畳み込み計算\n","        for i in range(len(w) - 1):\n","            a.append((x[i:i+len(w)] @ w) + b[0])\n","            \n","        return np.array(a)\n","    \n","    def backward(self, x, w, da):\n","        \"\"\"逆伝播\n","        x : 入力配列\n","        w : 重み\n","        da : 逆伝播の値　⇨順伝播の値でないのか？\n","        \"\"\"\n","        # バイアスの勾配\n","        db = np.sum(da)\n","        \n","        # 重みの勾配\n","        dw = []\n","        for i in range(len(w)):\n","            dw.append(da @ x[i:i+len(da)])\n","        dw = np.array(dw)\n","        \n","        # 逆伝播の値\n","        dx = []\n","        # 逆畳込み計算用配列\n","        new_w = np.insert(w[::-1], 0, 0) #w[::-1]:範囲を全指定にしてマイナスステップにすると、要素がリバースする。\n","        #a = [1, 2, 3, 4, 5, 6, 7, 8, 9]  a[::-1] [9, 8, 7, 6, 5, 4, 3, 2, 1]\n","        new_w = np.append(new_w, 0)\n","        for i in range(len(new_w)-1):\n","            dx.append(new_w[i:i+len(da)] @ da)\n","        dx = np.array(dx[::-1])\n","        return db, dw, dx"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUtLacQB9ZSf"},"source":["【問題2】1次元畳み込み後の出力サイズの計算\n","畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n","\n","$$\n","N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n","$$\n","\n","$N_{out}$ : 出力のサイズ（特徴量の数）\n","\n","\n","$N_{in}$ : 入力のサイズ（特徴量の数）\n","\n","\n","$P$ : ある方向へのパディングの数\n","\n","\n","$F$ : フィルタのサイズ\n","\n","\n","$S$ : ストライドのサイズ\n","\n"]},{"cell_type":"code","metadata":{"id":"MVdUrxyj-A49","executionInfo":{"status":"ok","timestamp":1630564551223,"user_tz":-540,"elapsed":24,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["def output_size_calculation(n_in, F, P=0, S=1):\n","    \"\"\"出力サイズ計算\n","    n_in : 入力サイズ\n","    F : フィルターサイズ\n","    P : パッディング数\n","    S : ストライド数\n","    \"\"\"\n","    # 出力サイズの計算\n","    n_out = int((n_in + 2*P - F) / S + 1)\n","    \n","    return n_out"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"sq1-t3CjDn3P","executionInfo":{"status":"ok","timestamp":1630564551223,"user_tz":-540,"elapsed":23,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":[""],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"311Rd9au9ZcB"},"source":["【問題3】小さな配列での1次元畳み込み層の実験\n","次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n","\n","$$\n"," ---------------\n","$$\n","入力x、重みw、バイアスbを次のようにします。\n","\n","\n","x = np.array([1,2,3,4])\n","\n","w = np.array([3, 5, 7])\n","\n","b = np.array([1])\n","\n","$$\n"," ---------------\n","$$\n","\n","フォワードプロパゲーションをすると出力は次のようになります。\n","\n","\n","a = np.array([35, 50])\n","\n","次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n","\n","\n","\n","delta_a = np.array([10, 20])\n","\n","バックプロパゲーションをすると次のような値になります。\n","\n","delta_b = np.array([30])\n","\n","delta_w = np.array([50, 80, 110])\n","\n","delta_x = np.array([30, 110, 170, 140])\n","\n","\n","$$\n"," ---------------\n","$$\n","\n","\n","\n","\n","実装上の工夫\n","畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n","\n","$$\n","a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n","$$\n","\n","バイアス項は単純な足し算のため、重みの部分を見ます。\n","\n","$$\n","\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n","$$\n","\n","これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n","\n","\n","$$\n"," ---------------\n","$$\n","\n","x = np.array([1, 2, 3, 4])\n","\n","w = np.array([3, 5, 7])\n","\n","a = np.empty((2, 3))\n","\n","indexes0 = np.array([0, 1, 2]).astype(np.int)\n","\n","indexes1 = np.array([1, 2, 3]).astype(np.int)\n","\n","a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n","\n","a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n","\n","a = a.sum(axis=1)\n","\n","$$\n"," ---------------\n","$$\n","ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n","\n","\n","また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n","\n","\n","$$\n"," ---------------\n","$$\n","x = np.array([1, 2, 3, 4])\n","\n","indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n","\n","print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n","$$\n"," ---------------\n","$$\n","\n","このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n","\n","\n","畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n","\n","\n","《参考》\n","\n","\n","以下のページのInteger array indexingの部分がこの方法についての記述です。\n","\n","\n","Indexing — NumPy v1.17 Manual\n","\n"]},{"cell_type":"code","metadata":{"id":"yaxQfp-4-MhO","executionInfo":{"status":"ok","timestamp":1630564551223,"user_tz":-540,"elapsed":23,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["# 問題3　小さな配列での1次元畳み込み層の実験\n","# テストデータ\n","x = np.array([1,2,3,4])\n","w = np.array([3, 5, 7])\n","b = np.array([1])\n","da = np.array([10, 20])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohlRN7srHgMM","executionInfo":{"status":"ok","timestamp":1630564551224,"user_tz":-540,"elapsed":23,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["# インスタンス化\n","simple_conv_1d = SimpleConv1d()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"936ZJp2BHgP8","executionInfo":{"status":"ok","timestamp":1630564551225,"user_tz":-540,"elapsed":24,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"0624cf79-0b6c-4448-d0ca-dd26a6975362"},"source":["# 順伝播\n","simple_conv_1d.forward(x, w, b)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([35, 50])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41mTsmEXHgSp","executionInfo":{"status":"ok","timestamp":1630564551226,"user_tz":-540,"elapsed":18,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"c42cccc7-b746-4274-fcc4-2814b98a5880"},"source":["# 逆伝播\n","db, dw, dx = simple_conv_1d.backward(x, w, da)\n","print(db)\n","print(dw)\n","print(dx)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["30\n","[ 50  80 110]\n","[ 30 110 170 140]\n"]}]},{"cell_type":"code","metadata":{"id":"8778NhC_HgWz","executionInfo":{"status":"ok","timestamp":1630564551226,"user_tz":-540,"elapsed":15,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dy47wwEuHgak","executionInfo":{"status":"ok","timestamp":1630564551227,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bq-CP6Gq9Zg_"},"source":["【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n","チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n","\n","\n","例えば以下のようなx, w, bがあった場合は、\n","\n","\n","x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n","w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n","b = np.array([1, 2, 3]) # （出力チャンネル数）\n","\n","出力は次のようになります。\n","\n","a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n","\n","入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n","\n","\n","《補足》\n","\n","\n","チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n","\n","\n","今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。\n","\n"]},{"cell_type":"code","metadata":{"id":"qLfEY0CP-N_O","executionInfo":{"status":"ok","timestamp":1630564551227,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["class Conv1d:\n","    \"\"\"畳み込みクラス\n","    \"\"\"\n","    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0):\n","        \"\"\"コンストラクタ\n","        b_size : フィルターサイズ\n","        initializer : 初期化クラス\n","        optimizer : 最適化手法クラス\n","        n_in_channels : 入力チャンネル数\n","        n_out_channels : 出力チャンネル数\n","        pa : パディング数\n","        \"\"\"\n","        self.b_size = b_size\n","        self.optimizer = optimizer\n","        self.pa = pa\n","        # 重みの初期化\n","        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n","        # バイアスの初期化\n","        self.B = initializer.B(n_out_channels)\n","        self.n_in_channels = n_in_channels\n","        self.n_out_channels = n_out_channels\n","        self.n_out = None\n","        \n","    def forward(self, X):\n","        \"\"\"順伝播\n","        X : 入力配列\n","        \"\"\"\n","        # 入力配列の特徴量数\n","        self.n_in = X.shape[-1]\n","        # 出力サイズ\n","        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa)\n","        # 計算のために逆転させる\n","        X = X.reshape(self.n_in_channels, self.n_in)\n","        # 0埋め実施\n","        self.X = np.pad(X, ((0,0), ((self.b_size-1), 0)))\n","        # 出力配列（A）の計算のためゼロ配列X1を用意する\n","        self.X1 = np.zeros((self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n","        # 重みの長さでループ\n","        for i in range(self.b_size):\n","            # ずらしながら上書き\n","            self.X1[:, i] = np.roll(self.X, -i, axis=-1)\n","        # 重みとバイアスを考慮して計算\n","        A = np.sum(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.B.reshape(-1,1)\n","        return A\n","    \n","    def backward(self, dA):\n","        \"\"\"逆伝播\n","        dA : 逆伝播してきた配列\n","        \"\"\"\n","        # 重みの勾配\n","        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\n","        # バイアスの勾配\n","        self.dB = np.sum(dA, axis=1)\n","        # 逆伝播の値計算のためにdAを変形\n","        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n","        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n","        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\n","        # 重みの長さでループ\n","        for i in range(self.b_size):\n","            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n","        dX = np.sum(self.W@self.dA1, axis=0)\n","        # 重みとバイアスの更新\n","        self.optimizer.update(self)\n","        return dX"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6pFjeZ7Vm-q","executionInfo":{"status":"ok","timestamp":1630564551227,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["# インスタンス化\n","conc_1d = Conv1d(b_size=3, initializer=SimpleInitializer(0.01), optimizer=SGD(0.01), n_in_channels=2, n_out_channels=3, pa=0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGxXkvpsVrJR","executionInfo":{"status":"ok","timestamp":1630564551228,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n","# コンストラクタ内で初期化しているけど確認のため再代入\n","conc_1d.W = np.ones((3, 2, 3), dtype=float)\n","conc_1d.B = np.array([1, 2, 3], dtype=float)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMNbzjwcVwGR","executionInfo":{"status":"ok","timestamp":1630564551228,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"04fd5b83-ef0e-4555-ec4d-f64b28992cf9"},"source":["conc_1d.forward(x)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[16., 22.],\n","       [17., 23.],\n","       [18., 24.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"dsqWfg8X9Z21"},"source":["【問題8】学習と推定\n","これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n","\n","\n","出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n","\n","\n","画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"]},{"cell_type":"code","metadata":{"id":"taUO6nYT-afZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630564552309,"user_tz":-540,"elapsed":1092,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"12556ac8-9ed7-4c2e-f52e-436f112e5a3d"},"source":["# データ読み込み\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# 画像データ→行データに\n","X_train = X_train.reshape(-1, 784)\n","X_test = X_test.reshape(-1, 784)\n","\n","# 正規化\n","X_train = X_train.astype(np.float)\n","X_test = X_test.astype(np.float)\n","X_train /= 255\n","X_test /= 255\n","\n","# onehotベクトル化\n","enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n","y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n","\n","# 訓練データと評価データに\n","X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"wl0fXL1UWxnv","executionInfo":{"status":"ok","timestamp":1630564552309,"user_tz":-540,"elapsed":5,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["class Conv1d_Arbitrary_Strides:\n","    \"\"\"畳み込みクラス\n","    \"\"\"\n","    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n","        \"\"\"コンストラクタ\n","        b_size : フィルターサイズ\n","        initializer : 初期化クラス\n","        optimizer : 最適化手法クラス\n","        n_in_channels : 入力チャンネル数\n","        n_out_channels : 出力チャンネル数\n","        pa : パディング数\n","        stride : ストライド数\n","        \"\"\"\n","        self.b_size = b_size\n","        self.optimizer = optimizer\n","        self.pa = pa\n","        self.stride = stride\n","        # 重みの初期化\n","        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n","        # バイアスの初期化\n","        self.B = initializer.B(n_out_channels)\n","        self.n_in_channels = n_in_channels\n","        self.n_out_channels = n_out_channels\n","        self.n_out = None\n","        \n","    def forward(self, X):\n","        \"\"\"順伝播\n","        X : 入力配列\n","        \"\"\"\n","        # バッチ数\n","        self.n_samples = X.shape[0]\n","        # 入力配列の特徴量数\n","        self.n_in = X.shape[-1]\n","        # 出力サイズ\n","        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n","        # 計算のためにX変形\n","        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n","        # 0埋め実施\n","        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n","        # 出力配列（A）の計算のためゼロ配列X1を用意する\n","        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n","        # 重みの長さでループ\n","        for i in range(self.b_size):\n","            # ずらしながら上書き\n","            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n","        # 重みとバイアスを考慮して計算\n","        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n","        return A\n","    \n","    def backward(self, dA):\n","        \"\"\"逆伝播\n","        dA : 逆伝播してきた配列\n","        \"\"\"\n","        # 重みの勾配\n","        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n","        # バイアスの勾配\n","        self.dB = np.sum(dA, axis=(0, -1))\n","        # 逆伝播の値計算のためにdAを変形\n","        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n","        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n","        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n","        # 重みの長さでループ\n","        for i in range(self.b_size):\n","            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n","        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n","        # 重みとバイアスの更新\n","        self.optimizer.update(self)\n","        return dX"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbDhRh5ZV_uy","executionInfo":{"status":"ok","timestamp":1630564552310,"user_tz":-540,"elapsed":5,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["class ScratchCNNClassifier:\n","    \n","    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n","        \"\"\"コンストラクタ\n","        Parameters\n","        -----------\n","        num_epoch : 学習回数\n","        lr : 学習率\n","        batch_size : バッチサイズ\n","        n_features : 特徴量数\n","        n_nodes1 : 1層目のノード数\n","        n_nodes2 : 2層目のノード数\n","        n_output : 出力層の数\n","        verbose : 仮定出力するか否か\n","        Activater : 活性化関数\n","        Optimizer : 最適化手法\n","        \"\"\"\n","        self.num_epoch = num_epoch\n","        self.lr = lr\n","        self.verbose = verbose  \n","        self.batch_size = batch_size \n","        self.n_features = n_features \n","        self.n_nodes2 = n_nodes2 \n","        self.n_output = n_output \n","        self.Activater = Activater\n","        if Activater == Sigmoid or Activater == Tanh:\n","            self.Initializer = XavierInitializer\n","        elif Activater == ReLU:\n","            self.Initializer = HeInitializer\n","        self.Optimizer = Optimizer\n","    \n","    def fit(self, X, y, X_val=None, y_val=None):\n","        \"\"\"学習\n","        Parameters\n","        ----------\n","        X : 訓練データの説明変数\n","        y : 訓練データの目的変数\n","        X_val : 評価データの説明変数\n","        y_val : 評価データの目的変数\n","        \"\"\"        \n","        # レイヤー初期化\n","        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_Strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n","        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculation(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\n","        self.activation1 = self.Activater()\n","        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n","        self.activation2 = self.Activater()\n","        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n","        self.activation3 = Softmax()\n","        \n","        # loss配列定義と初期値格納（loss:ミニバッチごとの損失格納  loss_epoch:ミニバッチ学習終了後の全体損失）\n","        self.loss = []\n","        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n","        \n","        # 学習回数分ループ\n","        for _ in range(self.num_epoch):\n","            # ミニバッチイテレータ生成\n","            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n","            # イテレータ呼び出し\n","            for mini_X, mini_y in get_mini_batch:\n","                # 順伝播\n","                self.forward_propagation(mini_X)\n","                # 逆伝播\n","                self.back_propagation(mini_X, mini_y)\n","                # 損失記録\n","                self.loss.append(self.activation3.loss)\n","            # 損失記録\n","            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n","    \n","    def predict(self, X):\n","        \"\"\"予測値出力\n","        Parameters\n","        ----------\n","        X : 説明変数\n","        \"\"\"\n","        return np.argmax(self.forward_propagation(X), axis=1)\n","    \n","    def forward_propagation(self, X):\n","        \"\"\"順伝播\n","        Parameters\n","        ----------\n","        X : 訓練データの説明変数\n","        \"\"\"\n","        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\n","        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n","        Z1 = self.activation1.forward(A1)\n","        A2 = self.FC2.forward(Z1)\n","        Z2 = self.activation2.forward(A2)\n","        A3 = self.FC3.forward(Z2)\n","        Z3 = self.activation3.forward(A3)\n","        return Z3\n","        \n","    def back_propagation(self, X, y_true):\n","        \"\"\"逆伝播\n","        Parameters\n","        ----------\n","        X : 訓練データの説明変数\n","        y_true : 正解データ\n","        \"\"\"\n","        dA3 = self.activation3.backward(y_true) \n","        dZ2 = self.FC3.backward(dA3)\n","        dA2 = self.activation2.backward(dZ2)\n","        dZ1 = self.FC2.backward(dA2)\n","        dA1 = self.activation1.backward(dZ1)\n","        dA1 = dA1[:, np.newaxis]\n","        dZ0 = self.Conv1d_Arbitrary_Strides.backward(dA1) "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nzn-G64ZWGPL","executionInfo":{"status":"ok","timestamp":1630564561066,"user_tz":-540,"elapsed":8760,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["cnn = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n","cnn.fit(X_train_[:1000], y_train_[:1000])"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSUcTEqBWuZh","executionInfo":{"status":"ok","timestamp":1630564562597,"user_tz":-540,"elapsed":1545,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"a2fa0883-7f4d-4d65-f6e6-816d2d3516cb"},"source":["y_pred = cnn.predict(X_test)\n","accuracy_score(y_test, y_pred)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8934"]},"metadata":{},"execution_count":17}]}]}