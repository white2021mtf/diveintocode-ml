{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear_regression.ipynb","provenance":[],"collapsed_sections":["t5npQ06WrOa4"],"mount_file_id":"1g8KgWOz83CHdYdV6Lw0qwSTtRlZNciW8","authorship_tag":"ABX9TyPbTAYNX0uRsPqvm+OvoLS6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nibA5DB58AOy"},"source":["Sprintの目的\n","スクラッチを通して線形回帰を理解する\n","オブジェクト指向を意識した実装に慣れる\n","数式をコードに落とし込めるようにする\n","\n","\n","このパラメータを更新するための手法が、最小二乗法と最急降下法です。"]},{"cell_type":"markdown","metadata":{"id":"64DZwa5-8ArJ"},"source":["2.線形回帰スクラッチ\n","\n","線形回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n","\n","\n","以下に雛形を用意してあります。このScratchLinearRegressionクラスにコードを書き加えていってください。\n","\n","\n","雛形\n","\n","\n","class ScratchLinearRegression():\n","    \"\"\"\n","    線形回帰のスクラッチ実装\n","    \n","    Parameters\n","    ----------\n","    num_iter : int\n","      イテレーション数\n","    lr : float\n","      学習率\n","    no_bias : bool\n","      バイアス項を入れない場合はTrue\n","    verbose : bool\n","      学習過程を出力する場合はTrue\n","    \n","    Attributes\n","    ----------\n","    self.coef_ : 次の形のndarray, shape (n_features,)\n","      パラメータ\n","    self.loss : 次の形のndarray, shape (self.iter,)\n","      訓練データに対する損失の記録\n","    self.val_loss : 次の形のndarray, shape (self.iter,)\n","      検証データに対する損失の記録\n","    \"\"\"\n","    \n","    def __init__(self, num_iter, lr, no_bias, verbose):\n","        # ハイパーパラメータを属性として記録\n","        self.iter = num_iter\n","        self.lr = lr\n","        self.no_bias = no_bias\n","        self.verbose = verbose\n","        # 損失を記録する配列を用意\n","        self.loss = np.zeros(self.iter)\n","        self.val_loss = np.zeros(self.iter)\n","        \n","    def fit(self, X, y, X_val=None, y_val=None):\n","        \"\"\"\n","        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            訓練データの特徴量\n","        y : 次の形のndarray, shape (n_samples, )\n","            訓練データの正解値\n","        X_val : 次の形のndarray, shape (n_samples, n_features)\n","            検証データの特徴量\n","        y_val : 次の形のndarray, shape (n_samples, )\n","            検証データの正解値\n","        \"\"\"\n","        if self.verbose:\n","            #verboseをTrueにした際は学習過程を出力\n","            print()\n","        pass\n","        \n","    def predict(self, X):\n","        \"\"\"\n","        線形回帰を使い推定する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","        Returns\n","        -------\n","            次の形のndarray, shape (n_samples, 1)\n","            線形回帰による推定結果\n","        \"\"\"\n","        pass\n","        return\n"]},{"cell_type":"markdown","metadata":{"id":"U9KWTxpv8hDW"},"source":["【問題1】仮定関数\n","以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n","\n","$$\n","h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n.   (x_0 = 1)\\\\\n","$$\n","\n","\n","\n","x\n"," : 特徴量ベクトル\n","\n","\n","θ\n"," : パラメータベクトル\n","\n","\n","n\n"," : 特徴量の数\n","\n","\n","x\n","j\n"," : j番目の特徴量\n","\n","\n","θ\n","j\n"," : j番目のパラメータ（重み）\n","\n","\n","特徴量の数\n","n\n","は任意の値に対応できる実装にしてください。\n","\n","\n","なお、ベクトル形式で表すと以下のようになります。\n","\n","\n","$$\n","h_\\theta(x) = \\theta^T \\cdot x.\n","$$\n","雛形\n","\n","\n","クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。\n","\n","\n","\n","def _linear_hypothesis(self, X):\n","    \"\"\"\n","    線形の仮定関数を計算する\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    Returns\n","    -------\n","      次の形のndarray, shape (n_samples, 1)\n","      線形の仮定関数による推定結果\n","    \"\"\"\n","    pass\n","    return\n"]},{"cell_type":"code","metadata":{"id":"I1ZTIea8jXsh","executionInfo":{"status":"ok","timestamp":1627908749787,"user_tz":-540,"elapsed":343,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#問題６に記載しました。\n"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_smOui-Zjrf1"},"source":["【問題2】最急降下法\n","最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n","メソッドから呼び出すようにしてください。\n","\n","\n","$$\n","\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n","$$\n","\n","α\n"," : 学習率\n","\n","\n","i\n"," : サンプルのインデックス\n","\n","\n","j\n"," : 特徴量のインデックス\n","\n","\n","雛形\n","\n","\n","ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n","\n","\n","1\n","2\n","3\n","4\n","5\n","def _gradient_descent(self, X, error):\n","    \"\"\"\n","    説明を記述\n","    \"\"\"\n","    pass\n","\n","雛形として用意されたメソッドや関数以外でも必要があれば各自作成して完成させてください。雛形を外れても問題ありません。"]},{"cell_type":"code","metadata":{"id":"TPHTStBGSyW8","executionInfo":{"status":"ok","timestamp":1627908750162,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#問題６に記載しました。"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t5npQ06WrOa4"},"source":["**これを数式で表すと下記のようになります。**\n","\n","$$\n","J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n","$$\n","\n","$$\n","\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n","$$\n","\n","\n","### 損失関数（目的関数）\n","\n","平均2乗誤差を最小にするΘを求めてきましたが、この対象となる誤差関数のことを、**損失関数（目的関数）**といいます。\n","\n","Θの更新の際に、平均2乗誤差を使用していると紹介しましたが、厳密には、展開後の式を分かりやすくするため、下記の数式を利用しています。\n","\n","$$\n","L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n","$$\n","\n","↓\n","\n","$$\n","J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n","$$\n","\n","どちらもΘについて微分し、係数を学習率に飲み込ませることで等価な式となります。"]},{"cell_type":"markdown","metadata":{"id":"t_NM4mzf8AyX"},"source":["【問題3】推定\n","推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n","\n","\n","仮定関数 \n","h\n","θ\n","(\n","x\n",")\n"," の出力が推定結果です。"]},{"cell_type":"code","metadata":{"id":"ZEaEak2GjZOc","executionInfo":{"status":"ok","timestamp":1627908750163,"user_tz":-540,"elapsed":14,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#問題６に記載しました"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sj4oxr9q8A2u"},"source":["【問題4】平均二乗誤差\n","線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n","\n","\n","平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n","\n","\n","平均二乗誤差は以下の数式で表されます。\n","\n","$$\n","L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n","$$\n","\n","\n","m\n"," : 入力されるデータの数\n","\n","\n","h\n","θ\n","(\n",")\n"," : 仮定関数\n","\n","\n","x\n","(\n","i\n",")\n"," : i番目のサンプルの特徴量ベクトル\n","\n","\n","y\n","(\n","i\n",")\n"," : i番目のサンプルの正解値\n","\n","\n","なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）\n","\n","\n","雛形\n","\n","def MSE(y_pred, y):\n","    \"\"\"\n","    平均二乗誤差の計算\n","    Parameters\n","    ----------\n","    y_pred : 次の形のndarray, shape (n_samples,)\n","      推定した値\n","    y : 次の形のndarray, shape (n_samples,)\n","      正解値\n","    Returns\n","    ----------\n","    mse : numpy.float\n","      平均二乗誤差\n","    \"\"\"\n","    pass\n","    return mse"]},{"cell_type":"markdown","metadata":{"id":"w2mJ_naOiJCU"},"source":["### 平均二乗誤差とは\n","平均二乗誤差とは教師データと予測値の残差を二乗し、訓練データ全ての点に対して足して平均を取ったものです。\n","\n","\n","式を分解して考えてみると、計算の順番は下記になるかと思います。\n","\n","1. 推定結果を計算\n","\n","$$\n","h_\\theta(x_i) = \\theta^T \\cdot x_i\n","$$\n","\n","2. 実測値との差を計算【対応する数式の個所を書いてください】\n","\n","$$\n","error_i = h_\\theta(x_i) - y_i\n","$$\n","\n","3. 1,2の2乗を計算\n","\n","$$\n","squared error_i = error_i^2\n","$$\n","\n","4. 3の合計値を計算【対応する数式の個所を書いてください】\n","\n","$$\n","sum squared error = \\sum_{i=1}^{m} squared error_i\n","$$\n","\n","5. データの長さで割って4の平均値を計算\n","\n","$$\n","mean squared error = \\sum_{i=1}^{m} squared error_i\n","$$\n","\n","ひな形では、推定結果と教師データが既に与えられるようにしています。\n","\n","ヒント：numpyの各種関数を利用してみましょう。\n","\n","### トイデータ\n","\n","作成した関数に、下記の変数を引数として与えてみましょう。\n","\n","```python\n","import numpy as np\n","y_pred = np.array([0,1,2,3,4,5])\n","y = np.array([1,3,5,7,9,11])\n","```\n","\n","戻り値として、下記の出力があれば、正常に作成できています。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phKu_WP9eYCv","executionInfo":{"status":"ok","timestamp":1627908750163,"user_tz":-540,"elapsed":13,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"264614a4-6731-4f68-cfd3-4d75f5887a5e"},"source":["# 問題4\n","import numpy as np\n","\n","def MSE(y_pred, Y):\n","      \"\"\"\n","      平均二乗誤差の計算\n","      Parameters\n","      ----------\n","      y_pred : 次の形のndarray, shape (n_samples,)\n","        推定した値\n","      y : 次の形のndarray, shape (n_samples,)\n","        正解値\n","      Returns\n","      ----------\n","      mse : numpy.float\n","        平均二乗誤差\n","      \"\"\"\n","      mse=0\n","      for i in  range(len(y_pred)):\n","        mse += ((y_pred[i] - y[i])**2)\n","      mse = mse/ len(y_pred)\n","      return mse\n","\n","y_pred = np.array([0,1,2,3,4,5])\n","y_pred=y_pred.tolist()\n","\n","y = np.array([1,3,5,7,9,11])\n","y=y.tolist()\n","\n","mse=MSE(y_pred, y)\n","print(mse)\n"],"execution_count":50,"outputs":[{"output_type":"stream","text":["15.166666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7gSy9C7uSaA","executionInfo":{"status":"ok","timestamp":1627908750164,"user_tz":-540,"elapsed":10,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"d5248446-c6bf-4086-a157-34891297e41d"},"source":["from sklearn.metrics import mean_squared_error\n","y_true = np.array([1,3,5,7,9,11])\n","y_pred = np.array([0,1,2,3,4,5])\n","mean_squared_error(y_true, y_pred)"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15.166666666666666"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"DSSDDv2E6EKT"},"source":["【問題４まとめ】\n","\n","スクラッチで書いたMSEの出力値とsklearn.metrics のmean_squared_error出力値が一致している"]},{"cell_type":"markdown","metadata":{"id":"XPcUX04t8A7i"},"source":["【問題5】目的関数\n","以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n","\n","\n","目的関数（損失関数） \n","J\n","(\n","θ\n",")\n"," は次の式です。\n","\n","\n","$$\n","J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n","$$\n","\n","\n","m\n"," : 入力されるデータの数\n","\n","\n","h\n","θ\n","(\n",")\n"," : 仮定関数\n","\n","\n","x\n","(\n","i\n",")\n"," : i番目のサンプルの特徴量ベクトル\n","\n","\n","y\n","(\n","i\n",")\n"," : i番目のサンプルの正解値\n","\n"," "]},{"cell_type":"code","metadata":{"id":"5JkXxob_pMWv","executionInfo":{"status":"ok","timestamp":1627908750164,"user_tz":-540,"elapsed":8,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#問題６に記載しました"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTdOO2sLpMiu"},"source":["【問題6】学習と推定\n","機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n","\n","\n","scikit-learnによる実装と比べ、正しく動いているかを確認してください。"]},{"cell_type":"code","metadata":{"id":"riuk_3VZpMzr","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1627908754172,"user_tz":-540,"elapsed":4015,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"c31d8296-973d-406d-f4e6-b67c46ac9d68"},"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import random\n","import matplotlib.pyplot as plt\n","\n","class ScratchLinearRegression():\n","    \"\"\"\n","    線形回帰のスクラッチ実装\n","    \n","    Parameters\n","    ----------\n","    num_iter : int\n","      イテレーション数\n","    lr : float\n","      学習率\n","    no_bias : bool\n","      バイアス項を入れない場合はTrue\n","    verbose : bool\n","      学習過程を出力する場合はTrue\n","    \n","    Attributes\n","    ----------\n","    self.coef_ : 次の形のndarray, shape (n_features,)\n","      パラメータ\n","    self.loss : 次の形のndarray, shape (self.iter,)\n","      訓練データに対する損失の記録\n","    self.val_loss : 次の形のndarray, shape (self.iter,)\n","      検証データに対する損失の記録\n","    \"\"\"\n","    \n","    def __init__(self, num_iter, lr, no_bias, verbose):\n","        # ハイパーパラメータを属性として記録\n","        self.iter = num_iter;\n","        self.lr = lr;\n","        self.no_bias = no_bias;\n","        self.verbose = verbose;\n","        # 損失を記録する配列を用意\n","        # self.loss = np.zeros(self.iter)\n","        # self.loss = self.loss.tolist()\n","        # self.val_loss = np.zeros(self.iter)\n","        self.theta = np.array([])  # 初期値\n","        self.theta2 = np.array([])  # 初期値\n","        self.loss = np.array([])  # 初期値\n","        self.val_loss = np.array([])  # 初期値\n","        # x = np.array( [] )\n","        self.iter2 = np.array([])  # 初期値\n","        self.iter2 = self.iter2.tolist()\n","      \n","\n","# 問題6（学習と推定）\n","    def fit(self, X, y, X_val, y_val):\n","        \"\"\"\n","        線形回帰の学習検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            訓練データの特徴量\n","        y : 次の形のndarray, shape (n_samples, )\n","            訓練データの正解値\n","        X_val : 次の形のndarray, shape (n_samples, n_features)\n","            検証データの特徴量\n","        y_val : 次の形のndarray, shape (n_samples, )\n","            検証データの正解値\n","        \"\"\"\n","        m = X.shape[0]\n","        hoge = np.ones((m, 1))\n","        X = np.hstack((hoge,X))\n","\n","        m2 = X_val.shape[0]\n","        hoge2 = np.ones((m2, 1))\n","        X_val = np.hstack((hoge2,X_val))\n","                            \n","        self.theta = np.zeros(X.shape[1]) #X.shape[1]：行列Xの列数を表す。np.zeros(X.shape[1])で、列数がX.shape[1]である０だけで構成される行列を作成している\n","        self.theta = self.theta.reshape(X.shape[1],1) #上記だと１次元配列なので、２次元にする為にreshape(Xの列数＋１（列が全て１の列）,1)にしている\n","        # print(\"self.theta\")\n","        # print(self.theta)\n","        y_true = y\n","        # print(y_true)\n","        # print(X)\n","        self.theta2 = np.zeros(X_val.shape[1]) #X.shape[1]：行列Xの列数を表す。np.zeros(X.shape[1])で、列数がX.shape[1]である０だけで構成される行列を作成している\n","        self.theta2 = self.theta2.reshape(X_val.shape[1],1) #上記だと１次元配列なので、２次元にする為にreshape(Xの列数＋１（列が全て１の列）,1)にしている\n","        # print(\"self.theta\")\n","        # print(self.theta)\n","        y_true2 = y_val\n","        # print(y_true)\n","        # print(X)\n","\n","        # メイン処理\n","        for k in range(self.iter):\n","            # print(\"イテレーション{}回目\".format(k))\n","\n","            #問題１解答\n","            h = self._linear_hypothesis(X)\n","            # print(\"h\")\n","            # print(h)\n","            h2 = self._linear_hypothesis(X_val)\n","            # print(\"h2\")\n","            # print(h2)\n","\n","            # 問題2解答（最急降下法によるパラメータの更新値計算）\n","            self.theta = self._gradient_descent(X,y_true,h)\n","            # print(\"self.theta\")\n","            # print(self.theta)\n","            self.theta2 = self._gradient_descent(X_val,y_true2,h2)\n","            # print(\"self.theta2\")\n","            # print(self.theta2)\n","           \n","            # 問題7解答（学習曲線のプロット）のグラフ描画時（問題5（損失関数）で作成した関数を使用）\n","            #損失関数グラフ化（学習データと検証データ比較）\n","            if(self.verbose ==True):\n","              loss= self._loss_func(h, y_true)\n","              loss2= self._loss_func(h2, y_true2)\n","              self.iter2.append(k)\n","              self.loss = np.append( self.loss, loss ) #物凄く時間がかかった所。（空ののself.lossにnumpy ndarrayの要素を追加する方法）\n","              self.val_loss = np.append( self.val_loss, loss2 )\n","        fig = plt.figure()\n","        ax = fig.add_subplot(111)\n","        plt.title(\"lossfunc\")\n","        plt.xlabel(\"iter\")\n","        plt.ylabel(\"loss\")\n","        ax.plot(self.iter2,self.loss, \"b\", label=\"train_loss\")\n","        ax.plot(self.iter2,self.val_loss, \"orange\", label=\"val_loss\")\n","        ax.grid(axis='both')\n","        # 凡例の表示\n","        ax.legend()\n","       \n","\n","\n","\n","    # 問題1\n","    def _linear_hypothesis(self,X):\n","        \"\"\"\n","        線形の仮定関数を計算する\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","          訓練データ\n","        Returns\n","        -------\n","          次の形のndarray, shape (n_samples, 1)\n","          線形の仮定関数による推定結果\n","        \"\"\"\n","        h = X @ self.theta\n","        return h\n","\n","    # 問題2解答\n","    def _gradient_descent(self,X,y_true,h):\n","        \"\"\"\n","        最急降下法によるパラメータの更新値計算\n","        適当な重みIrを掛けてθが小さくなる方へ徐々に転がっていくようにする\n","         X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","         y_true : 入力するモデルの学習に使用する正解値\n","        Returns\n","        -------\n","        hθ(X)=yhθ(X)=yからのズレを定量化してそのズレを最小化するようなθ\n","        次の値になるθ\n","        \"\"\"\n","        m=X.shape[0] #n_samplesサンプル数\n","        n=X.shape[1] #n_features特徴量の数\n","        for j in range(n):\n","          gradient = 0\n","          for i in range(m): #X.shape[0]は、Xの行数分を表す\n","            gradient += (h[i] - y_true[i])*X[i,j]\n","          self.theta[j] = self.theta[j] - self.lr /(m) * gradient \n","          \"\"\"\n","          ('210801に、この１行(上記のself.theta[j]・・・* gradient )のインデントが上（gradient += ・・・）と同じだった。\n","          そうすると住宅価格の予測値を出すと膨大な数値なった。原因は、このインデントが間違っていた。\n","          前に出して修正した。再急降下法の式を見ると、シグマ内にθはないので、前に出さないといけなかった。)\n","          \"\"\"\n","          # print(\"gradient\")\n","          # print(gradient)\n","        return self.theta\n","\n","        \n","\n","\n","    # 問題3解答\n","    def predict(self,X):\n","        \"\"\"\n","        線形回帰を使い推定する。\n","        Parameters\n","        ----------\n","        X : 次の形のndarray, shape (n_samples, n_features)\n","            サンプル\n","        Returns\n","        -------\n","            次の形のndarray, shape (n_samples, 1)\n","            線形回帰による推定結果\n","        \"\"\"\n","        m = X.shape[0]\n","        hoge = np.ones((m, 1))\n","        X = np.hstack((hoge,X))\n","                            \n","        y_pred = self._linear_hypothesis(X)\n","        return y_pred\n","\n","    # # 問題4\n","    # def _mse(self,y_pred, Y):\n","    #       \"\"\"\n","    #       平均二乗誤差の計算\n","    #       Parameters\n","    #       ----------\n","    #       y_pred : 次の形のndarray, shape (n_samples,)\n","    #         推定した値\n","    #       y : 次の形のndarray, shape (n_samples,)\n","    #         正解値\n","    #       Returns\n","    #       ----------\n","    #       mse : numpy.float\n","    #         平均二乗誤差\n","    #       \"\"\"\n","    #       mse = np.mean((y_pred - Y)**2)\n","    #       # pass\n","    #       return mse\n","\n","\n","    # 問題5解答\n","    def _loss_func(self,h, y_true):\n","      \"\"\"\n","      損失関数の計算\n","      Parameters\n","      ----------\n","      y_pred : 次の形のndarray, shape (n_samples,)\n","        推定した値\n","      y : 次の形のndarray, shape (n_samples,)\n","        正解値\n","      Returns\n","      ----------\n","      lossfunc\n","        損失値\n","      \"\"\"\n","      # print(\"y_true\")\n","      # print(y_true)\n","      m2=y_true.shape[0] #n_samplesサンプル数\n","      loss =0\n","      for i in  range(m2):\n","        loss += ((h[i] - y_true[i])**2)\n","      loss2 = loss/ (2*m2)\n","      # print(\"loss2\")\n","      # print(loss2)\n","      return loss2\n","\n","\n","\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/DIC/train.csv')\n","\n","\n","x=df.loc[:,['GrLivArea','YearBuilt']]\n","X=x.values\n","# print(x)\n","\n","y = df.loc[:, 'SalePrice']\n","y=y.values\n","# print(y)\n","\n","\n","np.random.seed(1234)\n","\n","\n","\n","#トレーニングデータとテストデータに分けて実行してみる------------------\n","X_train6, X_test6, train_label6, test_label6=train_test_split(X, y, train_size=0.8,random_state=0)\n","# print(\"X_train6\")\n","# print(X_train6.shape)\n","scaler = StandardScaler()\n","scaler.fit(X_train6)\n","X_train6=scaler.transform(X_train6)\n","X_test6=scaler.transform(X_test6)\n","\n","#ScratchLinearRegressionクラスのモデルのインスタンス化をregに代入\n","reg=ScratchLinearRegression(num_iter=100, lr=0.1, no_bias=True, verbose=True)\n","reg.fit(X_train6, train_label6, X_test6, test_label6)\n","\n","# print(X_test6)\n","\n","#予測値を求める\n","pre6=reg.predict(X_test6)\n","# print(pre6)\n","print(\"予測値 は、 \",'{:.1f}'.format(pre6.mean()))\n","\n","\n","\n"],"execution_count":53,"outputs":[{"output_type":"stream","text":["予測値 は、  181324.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"rhnqC_jmIexp"},"source":["【問題6】学習と推定の結果\n","\n","ScratchLinearRegressionクラスの予測値が約１８万になった。以下に記載した【scikit-learnによる実装】による予測値も約１８万で一致している。"]},{"cell_type":"code","metadata":{"id":"yH-aQm5Mt2xD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627908754173,"user_tz":-540,"elapsed":11,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}},"outputId":"c0525a12-e6ef-4ecd-9e7f-4861f0f8ad67"},"source":["#【scikit-learnによる実装】\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","# import os\n","# os.listdir()\n","# print(os.pwd)\n","df = pd.read_csv('/content/drive/MyDrive/DIC/train.csv')\n","# print(df)\n","\n","x=df.loc[:,['GrLivArea','YearBuilt']]\n","# print(type(x))\n","# print(x)\n","\n","target = df.loc[:, ['SalePrice']]\n","# print(target[:])\n","# print(type(target))\n","\n","X=x\n","X = X.sample(n=500, random_state=0)#サンプル数を5００へ絞り込み\n","X=X.values\n","# print(X)\n","\n","# 目的変数\n","Y = target\n","Y = Y.sample(n=500, random_state=0)#サンプル数を5００へ絞り込み\n","y6=Y.values\n","\n","\n","\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","rng = np.random.RandomState(0)\n","\n","\n","#トレーニングデータとテストデータに分けて実行してみる------------------\n","X_train6, X_test6, train_label6, test_label6=train_test_split(X, y6, train_size=0.8,random_state=0)\n","scaler = StandardScaler()\n","scaler.fit(X_train6)\n","X_train6=scaler.transform(X_train6)\n","\n","scaler.fit(X_test6)\n","X_test6=scaler.transform(X_test6)\n","\n","# LinearRegression\n","reg = LinearRegression()\n","reg.fit(X_train6, train_label6)\n","\n","#予測値を求める\n","pre6=reg.predict(X_test6)\n","# print(pre6)\n","# print(pre6.mean())\n","print(\"予測値 は、 \",'{:.1f}'.format(pre6.mean()))\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["予測値 は、  180294.7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yS0hLs_6pqRO"},"source":["【問題7】学習曲線のプロット\n","学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n","\n","\n","線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"r3BS-LbFJTfD","executionInfo":{"status":"ok","timestamp":1627908754173,"user_tz":-540,"elapsed":7,"user":{"displayName":"木村剛","photoUrl":"","userId":"13118920826758032389"}}},"source":["#問題６に記載しました"],"execution_count":55,"outputs":[]}]}